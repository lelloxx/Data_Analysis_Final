---
title: "main"
output: pdf_document
date: "2024-06-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(Metrics)
library(glmnet)
library(mgcv)
library(rpart)
library(e1071)
library(splines)
library(MASS)
library(dplyr)
library(corrplot)
library(ggplot2)
library(gridExtra)
```

Importing dataset and libraries
```{r df_import, include=FALSE}
a="/Users/alessandroausteri/Documents/GitHub/Data_Analysis_Final/BigMartSales.csv"
l="/Users/lorenzolaterza/Desktop/Data_Analysis_Final/BigMartSales.csv"


df= read.table(l,header=TRUE, sep=",") 
```
#EDA
##Data Cleaning

###Checking and Removing Duplicates                                             
```{r Removing Duplicates, include=FALSE}
# Check for entire row duplicates
duplicates_entire_row <- duplicated(df)

# Count the number of entirely duplicated rows
num_entire_row_duplicates <- sum(duplicates_entire_row)
print(paste("Number of duplicated rows:", num_entire_row_duplicates))
```
There aren’t any duplicates in our data set.

###Fixing Features of the Dataset

 
```{r unique values, echo=FALSE}
item_fat_content_uniques <- unique(df$Item_Fat_Content)#unique values of item_fat_content

print(item_fat_content_uniques)
```

```{r sistemare nomi item_fat_content, include=FALSE}
df$Item_Fat_Content <- ifelse(grepl("^[Rr]", df$Item_Fat_Content), "Regular Fat", "Low Fat")
item_fat_content_uniques <- unique(df$Item_Fat_Content)

print(item_fat_content_uniques)

```
If we go throughout the column `Item_Fat_Content` we can cearly see that there are some inconsistencies, in fact we have 2 different names for "Regular Fat" that are "Regular" and "reg" and 3 different names for "Low Fat" that are "Low fat", "low fat", "LF".
To solve this problem, we edited the column "Item_Fat_Content" in this way: all records that start with R or r, will be renamed "Regular Fat", and other will be renamed "Low Fat".
Then we will print the uniques name of the column after the edit to chek if the process ended successfully 

###Checking for Missing Values

```{r check for missing values, echo=FALSE}

# Crea una copia del dataframe convertendo tutte le colonne in stringhe
df_copy <- lapply(df, as.character)

# Creiamo una funzione per contare "NA", "" o "0" in ogni colonna della copia
count_na_empty_or_zero_strings <- function(column) {
  # Sostituiamo i veri NA con stringhe vuote per una conta coerente
  column[is.na(column)] <- ""
  sum(column == "" | column == "0", na.rm = TRUE)
}

# Applicare la funzione a tutte le colonne della copia e stampare i risultati
na_empty_or_zero_counts <- sapply(df_copy, count_na_empty_or_zero_strings)

# Stampa il numero di "NA", "" o "0" per ogni colonna della copia
print(na_empty_or_zero_counts)
```
We see that there are 3 columns with missing values: `Item_Weight`, `Item_Visibility` and `Outlet_Size`. We will start to handling missing values from `Outlet_Size`

```{r Outlet size analysis, include=FALSE}
#conteggio records senza Outlet_Size:
null<-sum(nchar(df$Outlet_Size) ==0)

#numero di records
total<- nrow(df)

print(paste("Number of rows with missing valuess on Outlet_Size:",null))

#percentuale di righe senza outlet_size
percentuale_stringhe_vuote <- (null / total) * 100

#stampa percentuale di righe senza outlet_size

print(paste("Percentuale di records con stringhe vuote in 'Outlet_Size':", percentuale_stringhe_vuote, "%"))
```

```{r updating outlet_size, include=FALSE}
# Aggiornamento della colonna 'Outlet_Size'
df <- df %>%
  mutate(Outlet_Size = case_when(
    Outlet_Type == "Grocery Store" ~ "Small",
    #Outlet_Type %in% c("Supermarket Type2", "Supermarket Type3") ~ "Medium",
    TRUE ~ Outlet_Size  # Mantiene il valore originale per tutte le altre condizioni
  ))

# Visualizza le modifiche per confermare
# Creazione di una tabella di riepilogo
outlet_summary <- df %>%
  filter(Outlet_Size %in% c("Small", "Medium", "High")) %>%  # Filtra per includere solo le righe con i valori specificati
  group_by(Outlet_Type, Outlet_Size) %>%  # Raggruppa per tipo e dimensione del negozio
  summarise(Count = n(), .groups = 'drop')  # Calcola il conteggio e rimuove il raggruppamento automatico

# Visualizzazione della tabella di riepilogo
print(outlet_summary)

```

```{r Outlet_Size Analysis 2, include=FALSE}
#conteggio records senza Outlet_Size:
null<-sum(nchar(df$Outlet_Size) ==0)

#numero di records
total<- nrow(df)

print(paste("Number of rows with missing values on `Outlet_Size now`:",null))

#percentuale di righe senza outlet_size
percentuale_stringhe_vuote <- (null / total) * 100

#stampa percentuale di righe senza outlet_size

print(paste("Percentage of missing values on `Outlet_Size` now:", percentuale_stringhe_vuote, "%"))
```

```{r adding na values, include=FALSE}
# Aggiornamento della colonna 'Outlet_Size' per riempire le stringhe vuote
df <- df %>%
  mutate(Outlet_Size = if_else(nchar(Outlet_Size) == 0, "NA", Outlet_Size))
```
To address the significant number of missing values were noted in the `Outlet_Size` column, we first calculated the percentage of missing entries to understand the scope of the issue.

Subsequently, we explored potential relationships between `Outlet_Type` and `Outlet_Size` by creating a cross-tabulation of these variables. This analysis revealed distinct patterns: all entries categorized as "Grocery Store" were consistently labeled as "Small", while "Supermarket Type2" and "Supermarket Type3" were uniformly classified as "Medium".

Considering this insight, we proceeded to change missing `Outlet_Size` values based on `Outlet_Type`:

For "Grocery Store", missing sizes were filled with "Small".
For "Supermarket Type2" and "Supermarket Type3", missing sizes were filled with "Medium".
These imputations reduced the percentage of missing values from 28% to 21%, decreasing the total number of missing entries from 2410 to 1855.

Regarding "Supermarket Type1" we do not have enough informations or clear patterns, so we had to fill the remaining missing entries with a placeholder value of "NA", ensuring no data point within "Outlet_Size" remained unaddressed.

```{r factor converting, include=FALSE}

# Conversione di 'Outlet_Size' in valori numerici
df <- df %>%
  mutate(Outlet_Size = case_when(
    Outlet_Size == "Small" ~ 1,
    Outlet_Size == "Medium" ~ 2,
    Outlet_Size == "High" ~ 3,
    TRUE ~ NA_real_  # Imposta NA per qualsiasi altro valore non specificato
  ))

df <- df%>%
  mutate(Item_Fat_Content = case_when(
    Item_Fat_Content == "Low Fat" ~ 1,
    Item_Fat_Content == "Regular Fat" ~ 2,
    TRUE ~ NA_real_
  ))
head(df)
```
Before proceeding with hndling missing values for the other features, we converted as factor the columns `Item_Fat_Content` and `Outlet_Size` giving respectively:
0 -> "Low Fat" 
1 -> "Regular"

1 -> Small
2 -> Medium
3 -> Large

```{r handling missing item_weight, include=FALSE}

# Calcolare la media del peso per ogni categoria di prodotto
average_weight_per_type <- aggregate(Item_Weight ~ Item_Type, data = df, mean, na.rm = TRUE)

# Funzione per riempire i pesi mancanti
fill_missing_weights <- function(item_id, item_type) {
  # Controlla se esiste un valore non NA per lo stesso Item_Identifier
  if (any(!is.na(df$Item_Weight[df$Item_Identifier == item_id]))) {
    return(df$Item_Weight[df$Item_Identifier == item_id & !is.na(df$Item_Weight)][1])
  } else {
    # Altrimenti usa la media del Item_Type corrispondente
    return(average_weight_per_type$Item_Weight[average_weight_per_type$Item_Type == item_type])
  }
}

# Applicare la funzione ai valori NA in Item_Weight
df$Item_Weight[is.na(df$Item_Weight)] <- mapply(fill_missing_weights, df$Item_Identifier[is.na(df$Item_Weight)], df$Item_Type[is.na(df$Item_Weight)])

# Visualizzare il dataframe aggiornato
head(df)


```
To handle missing values for `Item_Weight` our first idea was to use the general mean of `Item_Weight`, but since we have also the category of each product, we can compute the mean of every product category, so we checked for every record with missing `Item_Weight` and we double checked:
If there were another item with the same ID and missing weight, they will have for sure the same weight, so we substituted  in `Item_Weight`, the mean of the weights of the products of the same category.

#### Handling with item visibility

```{r visibility plot, include=False}


p4 <- ggplot(df, aes(x = "", y = Item_Visibility)) + 
  geom_boxplot(fill = 'green', alpha = 0.7) + 
  ggtitle("Box Plot of Item Visibility") +
  xlab("") +
  ylab("Item Visibility")
print(p4)
```

```{r count 0 values in item visibility, include=FALSE}
# Find the number of zero 'Item_Visibility' values for each 'Outlet_Identifier'
zero_visibility_counts <- aggregate(Item_Visibility ~ Outlet_Identifier, data = df, function(x) sum(x == 0))

# Rename the column for better understanding
names(zero_visibility_counts)[2] <- "Zero_Item_Visibility_Count"

# Display the result
print(zero_visibility_counts)
```

```{r count Visibility for each store 2, include=FALSE}

visibility_sum_per_store <- aggregate(Item_Visibility ~ Outlet_Identifier, data = df, sum)
names(visibility_sum_per_store)[2] <- "Total_Item_Visibility"

# Display the result
print(visibility_sum_per_store)
```

```{r Visibility}

# Filtrare i record con visibilità maggiore di zero per calcolare le medie
filtered_df <- df[df$Item_Visibility > 0, ]

# Calcolare la media di Item_Visibility per ogni combinazione di Item_Type e Outlet_Size
visibility_avg_per_type_size <- aggregate(Item_Visibility ~ Item_Type + Outlet_Size, data = filtered_df, mean)

# Creare una chiave unica per facilitare il merge
visibility_avg_per_type_size$key <- with(visibility_avg_per_type_size, paste(Item_Type, Outlet_Size, sep = "_"))
df$key <- with(df, paste(Item_Type, Outlet_Size, sep = "_"))

# Merge tra i record del dataframe originale e le medie calcolate usando un left join
df <- merge(df, visibility_avg_per_type_size, by = "key", all.x = TRUE, suffixes = c("", ".new"))

# Sostituire i valori zero di Item_Visibility con i valori medi calcolati
df$Item_Visibility[df$Item_Visibility == 0] <- df$Item_Visibility.new[df$Item_Visibility == 0]

# Rimuovere le colonne in più create dal merge
df <- df[, !names(df) %in% c("Item_Visibility.new", "key")]

```
```{r dropping useless }
df <- df %>%
  select(-Item_Type.new, -Outlet_Size.new)
```

```{r count Visibility for each store, include=FALSE}

visibility_sum_per_store <- aggregate(Item_Visibility ~ Outlet_Identifier, data = df, sum)
names(visibility_sum_per_store)[2] <- "Total_Item_Visibility"

# Display the result
print(visibility_sum_per_store)
```

Finally we fixed the missing values for `Item_Visibility`. To do it we firstly count the number of missing values for each of the 10 shops and their visibility to understand how much visibility we are missing from each store. Our first idea was to subtract each visibility percentage from 100% and then equally divide the remaining visibility among the number of missing visibility product. In doing that we obtained biased and not consistent data because:

1. The filled visibility in each product was too high compared to the non-zeros values
2. After adding again all the visibility the result was 100% for each shop, but could also be true that in some shops some products are out of stock so the final visibility could be less than 100%

For this reason we decided to change our strategy and to fill the missing values we firstly find the missing `Item_Visibility`, then through the `Item_ID` we find all the shops that have for that product a real value, then we check the `Outlet_Size` of each shop and compare to the one of the market with missing value if it's the same we compute the mean throu all the visibility that match these criteria.

We did this because for example if the missing visibility is in a Medium shop, we cannot consider the same product visibility in a Small shop, because it's obviously higher, so we take into accounts a visibility to compute the mean only if the `Outlet_Size` is the same.

```{r unique categories of "Item_Type", echo=FALSE}
#finding out all the uniques


item_fat_content_uniques <- unique(df$Item_Type)#unique values of item_fat_content

print(item_fat_content_uniques)
```

```{r categories reduction of "Item_Type"}
df <- df %>%
  mutate(Item_Type_Reduced = case_when(
    Item_Type %in% c("Baking Goods", "Breads", "Breakfast", "Dairy", "Meat", "Seafood") ~ "Food Basics",
    Item_Type %in% c("Canned", "Frozen Foods", "Snack Foods", "Starchy Foods") ~ "Processed Foods",
    Item_Type %in% c("Hard Drinks", "Soft Drinks") ~ "Beverages",
    Item_Type %in% c("Health and Hygiene", "Household") ~ "Non-Food Items",
    Item_Type == "Fruits and Vegetables" ~ "Produce",
    Item_Type == "Others" ~ "Others",
    TRUE ~ "Others"  # Catch-all for any undefined categories
  ))

#rename the column item_type_reduced into item_type

#drop old item_Type column

df <- df %>%
  select(-Item_Type)


# Renaming the column using dplyr
df <- df %>%
  rename(Item_Type = Item_Type_Reduced)
```
To ensure more product type-based results rather than product-specific ones, we have categorized item_type_category into the following groups: Food Basics, Processed Foods, Beverages, Non-Food Items, Produce, and Others.

### Multivariate Analysis

```{R corr matrix, echo=FALSE}

# Calculate Spearman's rank correlation matrix again if not already calculated
cor_matrix <- cor(df %>% select(where(is.numeric)), 
                  method = "spearman", 
                  use = "pairwise.complete.obs")

# Visualize the correlation matrix with coefficients inside the circles
corrplot(cor_matrix, method = "circle", type = "upper", order = "hclust",
         tl.col = "black", tl.srt = 45, tl.cex = 0.8, cl.cex = 0.8,
         col = colorRampPalette(c("#6BAED6", "#FFFFFF", "#FD8D3C"))(200),
         addCoef.col = "black",  # Sets color of the coefficients to black (choose based on your color scheme)
         number.cex = 0.6)  # Adjust coefficient text size appropriately

```

After we generated the Correlation Matrix, we can see that the main positive correlation with our target variable `Item_Outlet_Sales` are with
`Outlet_Size` and `Item_MRP`. It seems that the bigger the outlet, the higher the sales and an higher MRP lead to an increase in sales of that product.
From the matrix we can also see that `Item_Visibility` has a small negative correlation with `Item_Outlet_Sales` and `Outlet_Size`, suggesting that `Item_Visibility` doesn not impact sales, but we investigate this further later.

####Investigation of Correlated Features

```{r Plot most correlated features distributions, echo=FALSE, warning=FALSE}
# Specific price points to add to the plot for Item MRP vs Outlet Sales
specific_price_points <- c(67.99, 134.49, 199.99)

# Plot the relationship between Item MRP and Item Outlet Sales
p1 <- ggplot(df, aes(x = Item_MRP, y = Item_Outlet_Sales)) +
  geom_point(alpha = 0.5) +
  labs(title = "Outlet Sales per Item MRP",
       x = "Item MRP", y = "Outlet Sales")

# Add vertical lines for specific price points
for (price in specific_price_points) {
  p1 <- p1 + geom_vline(xintercept = price, linetype = "dotted", color = "blue", alpha = 0.7)
  p1 <- p1 + annotate("text", x = price, y = max(df$Item_Outlet_Sales) * 0.9, label = price, color = "blue", angle = 90, vjust = -0.5, size = 3)
}

# Convert selected categorical variables to factor if not already
df$Outlet_Size <- as.factor(df$Outlet_Size)

# Continuous Variables - Only consider Item Visibility for this demonstration
continuous_vars <- c("Item_Visibility")

# Categorical Variables - Only consider Outlet Size for this demonstration
categorical_vars <- c("Outlet_Size")

# Plotting Distribution of Sales for Continuous Variables
p2 <- ggplot(df, aes_string(x = continuous_vars, y = "Item_Outlet_Sales")) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "lm", color = "blue") +
    labs(title = paste( "Outlet Sales per", continuous_vars),
         x = continuous_vars, y = "Outlet Sales") +
    theme_minimal()

# Plotting Distribution of Sales for Categorical Variables
p3 <- df %>%
    group_by_(.dots = categorical_vars) %>%
    summarise(Average_Sales = mean(Item_Outlet_Sales)) %>%
    ggplot(aes_string(x = categorical_vars, y = "Average_Sales", fill = categorical_vars)) +
    geom_bar(stat = "identity", color = "black") +
    labs(title = paste("Average Sales by", categorical_vars),
         x = categorical_vars, y = "Average Sales") +
    theme_minimal()

# Ensure 'Item_Visibility' and 'Outlet_Sales' are treated correctly
df$Item_Visibility <- as.numeric(as.character(df$Item_Visibility))
df$Item_Outlet_Sales <- as.numeric(as.character(df$Item_Outlet_Sales))

# Convert categorical variables to factors
df$Item_Type <- as.factor(df$Item_Type)
df$Outlet_Type <- as.factor(df$Outlet_Type)

# Segmented Regression by Item Type
item_type_models <- df %>%
  group_by(Item_Type) %>%
  do(model = lm(Item_Outlet_Sales ~ Item_Visibility, data = .))

# Creating scatter plots segmented by Outlet_Type
p4 <- ggplot(df, aes(x = Item_Visibility, y = Item_Outlet_Sales)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  facet_wrap(~ Outlet_Type) +
  labs(title = "Outlet Sales for Visibility & O.Type",
       x = "Item Visibility", y = "Outlet Sales") +
  theme_minimal()

# Arrange all plots in a grid
grid.arrange(p1, p2, p3, p4, ncol = 2)
```
Thanks this plots we can interpret some results we obtained:
1st Plot:
Here we have `Item_Outlet_Sales` per `Item_MRP` and it confirm what we already saw with the correlation matrix that as the MRP grows the number of sales become higher. But in this graph we can also see a curious fact: we have price gaps. In fact we calculated at which price the MRP "jumps" and we discover that it does so at all the prices at which usually shops sell their products (.99, .46 ecc...). In fact we know that a markering strategy is to sell products not for example at \$200 but at \$199.99

2nd Plot: 
Here we have `Item_Outlet_Sales` per `Item_Visibility`and even from the graph that the trend is the same of the correlation matrix (slightly negative), suggesting that items with greater visibility do not necessarily correlate with higher sales. This could seem strange as marketing logic would suggest that higher visibility should lead to higher sales. However, we found a few potential explanations for this:
1. Very high visibility might lead to saturation where the product becomes so commonplace that it loses its appeal.
2. If visibility metrics are high due to poor placement (like being at the end of an aisle or near the checkout where customers might ignore them), it might not convert into sales.
3. Some products might inherently require less visibility due to strong brand loyalty or necessity.

3rd Plot:
The bar chart illustrating average sales by outlet size reveals that medium-sized outlets have more sales than large outlets. This could be attributed to several factors. Firstly, medium-sized outlets might be situated in more accessible locations, enhancing their visibility and customer reach. Additionally, they might manage their inventory and space more efficiently, creating a more pleasant shopping environment that avoids the overwhelming scale of larger stores. We also have to consider that we have a lot of NA data from the size that can change the final result. But in conclusion we didn’t have lots of surprise as the grocery stores have the smallest number of sales as predicted

4th Plot:
Here our goal was to investigate more on the `Item_Visibility` so we calculated the `Item_Outlet_Sales` per `Item_Visibility` by `Outlet_Type` to see if maybe there was one or more supermarket types responsible for the negative correlation. After the results we clearly see that for each type the result does not change, especially for grocery store where maybe people already knows what they need to buy and so the visibility doesn't affect at all their choices

```{R Relationship between Item MRP and Outlet Sales by Product Type}
df_summary <- df %>%
  group_by(Item_Type, MRP_Bracket = cut(Item_MRP, breaks = seq(0, max(Item_MRP), by = 20))) %>%
  summarize(Average_Sales = mean(Item_Outlet_Sales), .groups = 'drop')

ggplot(data = df_summary, aes(x = MRP_Bracket, y = Average_Sales, fill = Item_Type)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Average Outlet Sales by MRP Bracket and Product Type",
       x = "MRP Bracket",
       y = "Average Outlet Sales") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
Here in this plot we investigate more the fact that as the MRP increases also the `Item_Outlet_Sales` does. Here we also divide the bars for each product categoryto see if there are some difference, but there aren’t any surprises.
This relationship could sound strange cause as we know as we increase prices, the number of sales should decrease. In this case this might suggest that higher MRP might be associated with premium products that bring in more sales revenue.

```{R item outlet sales by outlet size, include=FALSE}
ggplot(df, aes(x = Outlet_Size, y = Item_Outlet_Sales)) +
  geom_boxplot(aes(fill = Outlet_Size)) +
  labs(title = "Item Outlet Sales by Outlet Size",
       x = "Outlet Size", y = "Outlet Sales")
```

```{R Sales trends over the years, include=FALSE}
ggplot(df, aes(x = Outlet_Establishment_Year, y = Item_Outlet_Sales)) +
  geom_point(aes(color = Outlet_Establishment_Year)) +
  geom_smooth(method = "lm") +
  labs(title = "Sales Trends Over the Years",
       x = "Establishment Year", y = "Outlet Sales")

```

```{R  Item_Outlet_Sales density plot, include=FALSE}
# Create a density plot
ggplot(data = df, aes(x = Item_Outlet_Sales)) +
  geom_density(fill = "turquoise", alpha = 0.5) +  # 'alpha' controls transparency
  labs(title = "Density Plot of Item Outlet Sales",
       x = "Item Outlet Sales",
       y = "Density") +
  theme_light()
```

```{r Linear model to investigate on Item Visibility and Sales, include=FALSE}
# Fit a linear model
model <- lm(Item_Outlet_Sales ~ Item_Visibility + Item_Type + Outlet_Type + Item_Fat_Content + Item_MRP, data = df)

# Summary of the model to understand influences
summary(model)
```
To further investigate which factors affecting the sales of items across various outlets we developed a linear model. The results suggest significant influences from the type of outlet and the `Item_MRP`, confirming what we previously analyzed. As we also found before, `Item_Visibility` and most `Item_Type` categories did not show a significant impact on sales, indicating these factors are not crucial in determining sales volumes. 
The F-statistic of 455.5 on 24 and 8498 degrees of freedom, along with a p-value of less than 2.2e-16, suggests that the model is statistically significant, suggesting that the model, while not perfect, captures a significant portion of the variation in sales across different outlets. This was not a predictive model, but was useful for us to confirm our analysis

```{R item visibility vs item mrp scatterplot, include=FALSE}
# Create a scatter plot
ggplot(df, aes(x = Item_MRP, y = Item_Visibility)) +
  geom_point(alpha = 0.5) +  # Use semi-transparent points to handle overplotting
  labs(x = "Item Maximum Retail Price (MRP)",
       y = "Item Visibility",
       title = "Relationship between Item MRP and Item Visibility") +
  theme_minimal()  # Clean minimalistic theme
```

```{R item visibility vs item mrp line graph, include=FALSE}
# Create bins for Item MRP using a reasonable interval
df$MRP_Bin <- cut(df$Item_MRP, breaks=seq(from=min(df$Item_MRP), to=max(df$Item_MRP), by=20), include.lowest=TRUE, right=TRUE)

# Calculate average visibility per MRP bin
average_visibility_per_mrp <- aggregate(Item_Visibility ~ MRP_Bin, data = df, mean)

# Create a line graph
ggplot(average_visibility_per_mrp, aes(x = MRP_Bin, y = Item_Visibility, group=1)) +
  geom_line() +  # Adds a line graph
  geom_point() +  # Adds points to each average point
  labs(x = "Item MRP Range", y = "Average Item Visibility",
       title = "Average Item Visibility Across Different MRP Ranges") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Improve x-axis label readability
```
Finally we investigate the relationship between `Item_Visibility` and `Item_MRP` to see if there was something that influence the visibility and we saw that the products with higher visibility are the ones with lowest MRP (the products that were not sold easily) and the ones with higher MRP (the products that could give more profit to the outlet). Here we have the final explanation of why the `Item_Visibility` is not affecting the number of sales as if a product has not done a considerable number of sales supermarkets tend to giv it more visibility to try to sell it more, but probably the problem is the product and how is it perceived from customers.

####Checking for Outliers

```{r plots to see outliers, include=FALSE}
# Load necessary libraries
library(ggplot2)
library(gridExtra)

# Histogram and Box Plot for Item_Weight
p1 <- ggplot(df, aes(x = Item_Weight)) + 
  geom_histogram(binwidth = 1, fill = 'blue', alpha = 0.7) + 
  ggtitle("Distribution of Item Weight") +
  xlab("Item Weight") +
  ylab("Frequency")

p2 <- ggplot(df, aes(x = "", y = Item_Weight)) + 
  geom_boxplot(fill = 'blue', alpha = 0.7) + 
  ggtitle("Box Plot of Item Weight") +
  xlab("") +
  ylab("Item Weight")

# Histogram and Box Plot for Item_Visibility
p3 <- ggplot(df, aes(x = Item_Visibility)) + 
  geom_histogram(binwidth = 0.01, fill = 'green', alpha = 0.7) + 
  ggtitle("Distribution of Item Visibility") +
  xlab("Item Visibility") +
  ylab("Frequency")

p4 <- ggplot(df, aes(x = "", y = Item_Visibility)) + 
  geom_boxplot(fill = 'green', alpha = 0.7) + 
  ggtitle("Box Plot of Item Visibility") +
  xlab("") +
  ylab("Item Visibility")

# Histogram and Box Plot for Item_MRP
p5 <- ggplot(df, aes(x = Item_MRP)) + 
  geom_histogram(binwidth = 5, fill = 'red', alpha = 0.7) + 
  ggtitle("Distribution of Item MRP") +
  xlab("Item MRP") +
  ylab("Frequency")

p6 <- ggplot(df, aes(x = "", y = Item_MRP)) + 
  geom_boxplot(fill = 'red', alpha = 0.7) + 
  ggtitle("Box Plot of Item MRP") +
  xlab("") +
  ylab("Item MRP")

# Histogram and Box Plot for Item_Outlet_Sales
p7 <- ggplot(df, aes(x = Item_Outlet_Sales)) + 
  geom_histogram(binwidth = 100, fill = 'purple', alpha = 0.7) + 
  ggtitle("Distribution of Item Outlet Sales") +
  xlab("Item Outlet Sales") +
  ylab("Frequency")

p8 <- ggplot(df, aes(x = "", y = Item_Outlet_Sales)) + 
  geom_boxplot(fill = 'purple', alpha = 0.7) + 
  ggtitle("Box Plot of Item Outlet Sales") +
  xlab("") +
  ylab("Item Outlet Sales")

# Print the plots individually
print(p1)
print(p2)
print(p3)
print(p4)
print(p5)
print(p6)
print(p7)
print(p8)

```
We verified the presence of outliers and observed some high values in item sales and item visibility. However, we concluded that these values are plausible and decided against removing them, as they likely represent genuine variations within our data.


```{r Item type vs Item_Sales, include=FALSE}

# Esegui ANOVA
avg_sales_by_type <- df %>%
  group_by(Item_Type) %>%
  summarise(Average_Sales = mean(Item_Outlet_Sales, na.rm = TRUE))

# Creating a bar graph of average sales by item type
ggplot(avg_sales_by_type, aes(x = Item_Type, y = Average_Sales, fill = Item_Type)) +
  geom_bar(stat = "identity", width = 0.7) +  # Using identity to use the heights of the bars to represent values in the data
  labs(title = "Average Sales by Item Type", x = "Item Type", y = "Average Sales") +
  theme_minimal() +  # Clean minimalistic theme
  theme(axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x labels for better visibility
        legend.title = element_blank()) +  # Remove legend title if not needed
  scale_fill_brewer(palette = "Paired")  # Optional: Use a color palette that is visually appealing

```
#### Lower Dimensional Models
```{r Impact of item MRP ad Outlet Type on Sales 2, include=FALSE}
model_mrp_outlet <- lm(Item_Outlet_Sales ~ Item_MRP * Outlet_Type, data = df)
summary(model_mrp_outlet)

# Plotting this relationship
ggplot(df, aes(x = Item_MRP, y = Item_Outlet_Sales, color = Outlet_Type)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~Outlet_Type) +
  labs(title = "Impact of MRP and Outlet Type on Sales")
```
We generated some plots across different outlet types (Grocery Store, Supermarket Type1, Type2, and Type3) show how Item_MRP influences sales and we also found that in Grocery Store the sales seem mostly flat regardless of the MRP, suggesting that price changes do not significantly influence sales in grocery stores. This could indicate price sensitivity or a limited range of products where price does not play a major role. In Supermarket Type1 there appears to be a moderate positive relationship between MRP and sales. This indicates that as the price of an item increases, sales also tend to increase, possibly due to a perception of higher quality or the availability of a wider range of products. For Supermarket Type2 is similar to Type1, but the relationship looks slightly weaker. This might be due to different customer demographics or store locations.While for Supermarket Type3 there is a strong positive relationship between MRP and sales, suggesting that customers at Type3 supermarkets are less price-sensitive and possibly more driven by product quality or brand.

```{r calculating Outlet Age, include=FALSE}
# Add a new column 'Outlet_Age' to the dataframe
df <- df %>%
  mutate(Outlet_Age = as.numeric(format(Sys.Date(), "%Y")) - Outlet_Establishment_Year)

# Display the age of each store
# This creates a summary table with Outlet_ID and its corresponding Outlet_Age
store_ages <- df %>%
  select(Outlet_Identifier, Outlet_Age) %>%  # Select the necessary columns
  distinct() %>%  # Remove duplicate rows to ensure each store is listed once
  arrange(Outlet_Identifier)  # Optional: Sort by Outlet Identifier for easier reading

# Print the resulting table to see each store's age
#print(store_ages)

# Aggregate total sales by Outlet_ID
sales_summary <- df %>%
  group_by(Outlet_Identifier) %>%
  summarise(Total_Sales = sum(Item_Outlet_Sales)) %>%
  ungroup()
# Merge sales summary with store ages
sales_with_age <- merge(sales_summary, store_ages, by = "Outlet_Identifier")
```
Before proceeding with the application of the models we transformed the `Outlet_Establishment_Year` into `Outlet_Age` to be more confortable in our analysis

##Model Implementation

```{R one hot econding, include=FALSE}
# Identificazione delle variabili categoriche per l'encoding
categorical_vars <- c("Outlet_Type", "Item_Type_Simplified")

# Applicazione del one-hot encoding
dummies <- dummyVars(~ Outlet_Type + Item_Type, data = df, fullRank = FALSE)
combined_data_ohe <- predict(dummies, newdata = df)

# Conversione in dataframe
combined_data_ohe <- as.data.frame(combined_data_ohe)

# Rimozione delle variabili categoriche originali e la colonna "Item_Type"
df <- df %>%
  select(-one_of(categorical_vars), -Item_Type) %>%
  bind_cols(combined_data_ohe)

```
We performed One-Hot encoding was applied to the `Outlet Type`" and `Item_Type` variables to transform these categorical attributes into a numerical format that predictive models can interpret and accurately assess the impact of different categories on the target variable. 


```{R splitting set, include=FALSE}
library(caret)

# Impostazione del seed per la riproducibilità
set.seed(123)

# Divisione del dataset in set di training e test
index <- createDataPartition(df$Item_Outlet_Sales, p = 0.8, list = FALSE)
train_data <- df[index, ]
test_data <- df[-index, ]
```
Then we splitted our dataset in train and test set, we used 80% and 20% respectively


```{R model on the train set, include=FALSE}
# Carico le librerie necessarie, se non già caricato
if (!require("ggplot2")) install.packages("ggplot2")
library(ggplot2)

# Assumo che il tuo dataframe si chiami train_data

# Modello per Item_MRP
train_data_MRP <- na.omit(train_data[, c("Item_MRP", "Item_Outlet_Sales")])  # Omette NA solo nelle colonne specificate
model_Item_MRP <- lm(Item_Outlet_Sales ~ Item_MRP, data = train_data_MRP)

# Modello per Outlet_Size
train_data_Size <- na.omit(train_data[, c("Outlet_Size", "Item_Outlet_Sales")])
model_Outlet_Size <- lm(Item_Outlet_Sales ~ Outlet_Size, data = train_data_Size)

# Modello per Item_Visibility
train_data_Visibility <- na.omit(train_data[, c("Item_Visibility", "Item_Outlet_Sales")])
model_Item_Visibility <- lm(Item_Outlet_Sales ~ Item_Visibility, data = train_data_Visibility)

# Modello per Outlet_Age
train_data_Age <- na.omit(train_data[, c("Outlet_Age", "Item_Outlet_Sales")])
model_Outlet_Age <- lm(Item_Outlet_Sales ~ Outlet_Age, data = train_data_Age)

# Stampa i riepiloghi dei modelli
cat("\nRiepilogo del modello per Item MRP:\n")
print(summary(model_Item_MRP))

cat("\nRiepilogo del modello per Outlet Size:\n")
print(summary(model_Outlet_Size))

cat("\nRiepilogo del modello per Item Visibility:\n")
print(summary(model_Item_Visibility))

cat("\nRiepilogo del modello per Outlet Age:\n")
print(summary(model_Outlet_Age))

```

```{R model on the test set, include=FALSE}

# Modello per Item_MRP sul test set
test_data_MRP <- na.omit(test_data[, c("Item_MRP", "Item_Outlet_Sales")])
predictions_MRP <- predict(model_Item_MRP, newdata = test_data_MRP)

# Modello per Outlet_Size sul test set
test_data_Size <- na.omit(test_data[, c("Outlet_Size", "Item_Outlet_Sales")])
predictions_Size <- predict(model_Outlet_Size, newdata = test_data_Size)

# Modello per Item_Visibility sul test set
test_data_Visibility <- na.omit(test_data[, c("Item_Visibility", "Item_Outlet_Sales")])
predictions_Visibility <- predict(model_Item_Visibility, newdata = test_data_Visibility)

# Modello per Outlet_Age sul test set
test_data_Age <- na.omit(test_data[, c("Outlet_Age", "Item_Outlet_Sales")])
predictions_Age <- predict(model_Outlet_Age, newdata = test_data_Age)

# Calcolo delle metriche di performance (RMSE e R-squared) per ogni modello
library(Metrics)

# RMSE
rmse_MRP <- rmse(test_data_MRP$Item_Outlet_Sales, predictions_MRP)
rmse_Size <- rmse(test_data_Size$Item_Outlet_Sales, predictions_Size)
rmse_Visibility <- rmse(test_data_Visibility$Item_Outlet_Sales, predictions_Visibility)
rmse_Age <- rmse(test_data_Age$Item_Outlet_Sales, predictions_Age)

# R-squared
r2_MRP <- summary(lm(Item_Outlet_Sales ~ predictions_MRP, data = test_data_MRP))$r.squared
r2_Size <- summary(lm(Item_Outlet_Sales ~ predictions_Size, data = test_data_Size))$r.squared
r2_Visibility <- summary(lm(Item_Outlet_Sales ~ predictions_Visibility, data = test_data_Visibility))$r.squared
r2_Age <- summary(lm(Item_Outlet_Sales ~ predictions_Age, data = test_data_Age))$r.squared

# Stampa dei risultati
print(data.frame(
  Variable = c("Item_MRP", "Outlet_Size", "Item_Visibility", "Outlet_Age"),
  RMSE = c(rmse_MRP, rmse_Size, rmse_Visibility, rmse_Age),
  R_Squared = c(r2_MRP, r2_Size, r2_Visibility, r2_Age)
))

```
Then we constructed lower-level models to identify which variables will be most effective for our final dataset. Intuitively, we suspected that `Item_MRP`, `Outlet_Size`, `Item Visibility` and `Outlet ID` might be good indicators for predicting our target variable, `Item_Outlet_Sales`, however, `Outlet_ID` couldn’t be directly converted into a numeric format because its values do not follow a numerical order, making it unsuitable for regression models that require numerical inputs.
As an alternative, we replaced it with `Outlet Age` (which ewe created before from `Outlet_Establishment_Year`) to examine whether the longevity of an outlet influences the sales of a product.
In our linear models code we addressed any missing values in these variables as linear models cannot be executed with missing data, so we removed any remaining NAs to ensure the integrity of our analysis.

Regarding the predictive power of each variable:
- **Item MRP** is highly predictive, explaining a significant portion of the variance in sales. This variable is likely to be a key component in the final model.
- **Outlet Size** shows low predictive power but might still offer insights when combined with other variables in a multivariate model.
- **Item Visibility** and **Outlet Age** appear to have negligible predictive ability when used alone. They might not significantly enhance a predictive model of sales.
So our analysis will be runned on these variables.
```{R stepwise regression, Include=FALSE}
if (!require(caret)) install.packages("caret")
if (!require(MASS)) install.packages("MASS")
library(caret)
library(MASS)

# Identificazione e rimozione delle variabili categoriche
train_data <- train_data[, sapply(train_data, is.numeric)]
test_data <- test_data[, sapply(test_data, is.numeric)]
test_data <- na.omit(test_data)  # Rimuove tutte le righe con NA
train_data <- na.omit(train_data)
# Modello iniziale con le variabili numeriche rimanenti
fit_initial <- lm(Item_Outlet_Sales ~ ., data = train_data)

# Selezione delle variabili usando il metodo backward
step_model <- stepAIC(fit_initial, direction = "backward", trace = FALSE)

# Stampa del modello finale dopo la selezione backward
print(summary(step_model))

# Previsione sul test set utilizzando il modello selezionato
predictions <- predict(step_model, newdata = test_data)

# Calcolo delle metriche di performance sul test set
actuals <- test_data$Item_Outlet_Sales
rmse_value <- sqrt(mean((predictions - actuals)^2))
r_squared <- summary(lm(actuals ~ predictions))$r.squared

# Stampa delle metriche di performance
cat("Performance Metrics on Test Set:\n")
cat("RMSE:", rmse_value, "\n")
cat("R-squared:", r_squared, "\n")

# Puoi anche desiderare di vedere come il modello si comporta sul set di training per confronto
train_predictions <- predict(step_model, newdata = train_data)
train_actuals <- train_data$Item_Outlet_Sales
train_rmse <- sqrt(mean((train_predictions - train_actuals)^2))
train_r_squared <- summary(lm(train_actuals ~ train_predictions))$r.squared

# Stampa delle metriche di performance sul train set
cat("Performance Metrics on Train Set:\n")
cat("RMSE:", train_rmse, "\n")
cat("R-squared:", train_r_squared, "\n")

```
```{r Extracting Stepwise Variables, include=FALSE}
# Extracting the names of the variables used in the final stepwise model
stepwise_variables <- names(coef(step_model))
stepwise_variables <- c(stepwise_variables, "")
stepwise_variables <- stepwise_variables[-c(1, length(stepwise_variables))]

# Print the list of variables used in the stepwise regression model
print(stepwise_variables)
```
After that we utilized stepwise regression to identify the most influential predictors for our target variable
```{R Ridge, include=FALSE }
if (!require(glmnet)) install.packages("glmnet")
library(glmnet)
# Preparazione dei dati
x_train <- model.matrix(Item_Outlet_Sales ~ . - 1, data = train_data)
y_train <- train_data$Item_Outlet_Sales

x_test <- model.matrix(Item_Outlet_Sales ~ . - 1, data = test_data)
y_test <- test_data$Item_Outlet_Sales
# Modello Ridge
set.seed(123)
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0, lambda = 10^seq(10, -2, length = 100))
best_lambda_ridge <- ridge_model$lambda.min

# Previsione sul test set
ridge_predictions <- predict(ridge_model, s = best_lambda_ridge, newx = x_test)

# Calcolo delle metriche di performance
ridge_rmse <- sqrt(mean((ridge_predictions - y_test)^2))
ridge_r_squared <- summary(lm(y_test ~ ridge_predictions))$r.squared

cat("Ridge Regression Performance Metrics:\n")
cat("RMSE:", ridge_rmse, "\n")
cat("R-squared:", ridge_r_squared, "\n")
```

```{r Ridge using only the variables selected from stepwise, include=FALSE}
if (!require(glmnet)) install.packages("glmnet")
library(glmnet)

formula <- as.formula(paste("Item_Outlet_Sales ~", paste(stepwise_variables, collapse=" + ")))

# Creating the model matrices
x_train <- model.matrix(formula, data=train_data)
y_train <- train_data$Item_Outlet_Sales

x_test <- model.matrix(formula, data=test_data)
y_test <- test_data$Item_Outlet_Sales

# Ridge Regression Model
set.seed(123)
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0, lambda = 10^seq(10, -2, length = 100))
best_lambda_ridge <- ridge_model$lambda.min

# Prediction on the test set
ridge_predictions <- predict(ridge_model, s = best_lambda_ridge, newx = x_test)

# Calculate performance metrics
ridge_rmse <- sqrt(mean((ridge_predictions - y_test)^2))
ridge_r_squared <- summary(lm(y_test ~ ridge_predictions))$r.squared

cat("Ridge Regression Performance Metrics:\n")
cat("RMSE:", ridge_rmse, "\n")
cat("R-squared:", ridge_r_squared, "\n")
```

```{R Lasso Regression, include=FALSE}
# Modello Lasso
set.seed(123)
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, lambda = 10^seq(10, -2, length = 100))
best_lambda_lasso <- lasso_model$lambda.min

# Previsione sul test set
lasso_predictions <- predict(lasso_model, s = best_lambda_lasso, newx = x_test)

# Calcolo delle metriche di performance
lasso_rmse <- sqrt(mean((lasso_predictions - y_test)^2))
lasso_r_squared <- summary(lm(y_test ~ lasso_predictions))$r.squared

cat("Lasso Regression Performance Metrics:\n")
cat("RMSE:", lasso_rmse, "\n")
cat("R-squared:", lasso_r_squared, "\n")
```

```{r lasso with only the selected variables by stepwise reg, include=FALSE}
if (!require(glmnet)) install.packages("glmnet")
library(glmnet)

formula <- as.formula(paste("Item_Outlet_Sales ~", paste(stepwise_variables, collapse=" + "), "- 1"))

# Creating the model matrices
x_train <- model.matrix(formula, data=train_data)
y_train <- train_data$Item_Outlet_Sales

x_test <- model.matrix(formula, data=test_data)
y_test <- test_data$Item_Outlet_Sales

# Lasso Regression Model
set.seed(123)
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, lambda = 10^seq(10, -2, length = 100))
best_lambda_lasso <- lasso_model$lambda.min

# Prediction on the test set
lasso_predictions <- predict(lasso_model, s = best_lambda_lasso, newx = x_test)

# Calculate performance metrics
lasso_rmse <- sqrt(mean((lasso_predictions - y_test)^2))
lasso_r_squared <- summary(lm(y_test ~ lasso_predictions))$r.squared

cat("Lasso Regression Performance Metrics:\n")
cat("RMSE:", lasso_rmse, "\n")
cat("R-squared:", lasso_r_squared, "\n")
```

```{R elastic net, include=FALSE}
set.seed(123)
elastic_net_model <- cv.glmnet(x_train, y_train, alpha = 0.5, lambda = 10^seq(10, -2, length = 100))
best_lambda_elastic_net <- elastic_net_model$lambda.min

# Previsione sul test set
elastic_net_predictions <- predict(elastic_net_model, s = best_lambda_elastic_net, newx = x_test)

# Calcolo delle metriche di performance
elastic_net_rmse <- sqrt(mean((elastic_net_predictions - y_test)^2))
elastic_net_r_squared <- summary(lm(y_test ~ elastic_net_predictions))$r.squared

cat("Elastic Net Performance Metrics:\n")
cat("RMSE:", elastic_net_rmse, "\n")
cat("R-squared:", elastic_net_r_squared, "\n")
```

```{r elastic net using only the variables selected by stepwise, include=FALSE}
if (!require(glmnet)) install.packages("glmnet")
library(glmnet)

# Using the 'stepwise_variables' list to define the formula for the model matrix
formula <- as.formula(paste("Item_Outlet_Sales ~", paste(stepwise_variables, collapse=" + "), "- 1"))

# Creating the model matrices
x_train <- model.matrix(formula, data=train_data)
y_train <- train_data$Item_Outlet_Sales

x_test <- model.matrix(formula, data=test_data)
y_test <- test_data$Item_Outlet_Sales

# Elastic Net Model
set.seed(123)
elastic_net_model <- cv.glmnet(x_train, y_train, alpha = 0.5, lambda = 10^seq(10, -2, length = 100))
best_lambda_elastic_net <- elastic_net_model$lambda.min

# Prediction on the test set
elastic_net_predictions <- predict(elastic_net_model, s = best_lambda_elastic_net, newx = x_test)

# Calculate performance metrics
elastic_net_rmse <- sqrt(mean((elastic_net_predictions - y_test)^2))
elastic_net_r_squared <- summary(lm(y_test ~ elastic_net_predictions))$r.squared

cat("Elastic Net Performance Metrics:\n")
cat("RMSE:", elastic_net_rmse, "\n")
cat("R-squared:", elastic_net_r_squared, "\n")
```
We performed Ridge regression, Lasso regression, and Elastic Net using both the full set of variables and only those selected through stepwise regression. Across all methods, the R-squared values consistently remained around 0.57, regardless of whether we utilized all predictors or a reduced feature set. This indicates that the explanatory power of our models is unchanged, despite the variety of techniques and the breadth of variables considered.

1. ensembling dei 3 metodi non lineari
2. ulterior feature engineering
3. Random Forest tree trovando il numero adeguato di alberi e di profondità

####Ensembling Methods
```{R ensembling / stacking, echo=FALSE}
set.seed(123)
x_train <- model.matrix(Item_Outlet_Sales ~ . - 1, data = train_data)
y_train <- train_data$Item_Outlet_Sales

x_test <- model.matrix(Item_Outlet_Sales ~ . - 1, data = test_data)
y_test <- test_data$Item_Outlet_Sales
# Modello Ridge
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0, lambda = 10^seq(10, -2, length = 100))
ridge_predictions_train <- predict(ridge_model, s = ridge_model$lambda.min, newx = x_train)
ridge_predictions_test <- predict(ridge_model, s = ridge_model$lambda.min, newx = x_test)

# Modello Lasso
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, lambda = 10^seq(10, -2, length = 100))
lasso_predictions_train <- predict(lasso_model, s = lasso_model$lambda.min, newx = x_train)
lasso_predictions_test <- predict(lasso_model, s = lasso_model$lambda.min, newx = x_test)

# Modello Elastic Net
elastic_net_model <- cv.glmnet(x_train, y_train, alpha = 0.5, lambda = 10^seq(10, -2, length = 100))
elastic_net_predictions_train <- predict(elastic_net_model, s = elastic_net_model$lambda.min, newx = x_train)
elastic_net_predictions_test <- predict(elastic_net_model, s = elastic_net_model$lambda.min, newx = x_test)
# Combinare le previsioni dei modelli base per il set di allenamento
stacked_predictions_train <- data.frame(
  Ridge = ridge_predictions_train,
  Lasso = lasso_predictions_train,
  ElasticNet = elastic_net_predictions_train
)

# Combinare le previsioni dei modelli base per il set di test
stacked_predictions_test <- data.frame(
  Ridge = ridge_predictions_test,
  Lasso = lasso_predictions_test,
  ElasticNet = elastic_net_predictions_test
)
# Addestrare il meta-learner (regressione lineare)
meta_learner <- lm(y_train ~ ., data = stacked_predictions_train)

# Previsioni del meta-learner sul set di test
meta_predictions_test <- predict(meta_learner, newdata = stacked_predictions_test)

# Calcolo delle metriche di performance
meta_rmse <- sqrt(mean((meta_predictions_test - y_test)^2))
meta_r_squared <- summary(lm(y_test ~ meta_predictions_test))$r.squared

cat("Stacking Performance Metrics:\n")
cat("RMSE:", meta_rmse, "\n")
cat("R-squared:", meta_r_squared, "\n")
```
We also performed ensembling/stacking, and as observed, the performance remains similar, with the R-squared value still around 0.57. Given that the performance did not improve significantly even with stacking, we will proceed to add new features to our model through another feature engineering to identify additional factors that could potentially enhance the predictive power and provide a better understanding of the variables influencing sales.

```{R feature engineering, include=FALSE}

# Creazione della variabile Price_Per_Unit_Weight
df$Price_Per_Unit_Weight <- ifelse(is.na(df$Item_Weight), 0, df$Item_MRP / df$Item_Weight)


# Creazione della variabile logaritmica per Item_Visibility
df$Log_Item_Visibility <- log(df$Item_Visibility + 1)

# Verifica delle prime righe del dataset per controllare le nuove variabili
sum(is.na(df$Item_MRP))
sum(is.na(df$Outlet_Size))
df <- na.omit(df, cols = "Outlet_Size")
df <- df %>%
  filter_all(all_vars(!(is.character(.) & . == "")))
df$Outlet_Size <- as.numeric((df$Outlet_Size))

df$MRP_x_Outlet_Size <- df$Item_MRP * df$Outlet_Size


```

```{R stepwise regression}
if (!require(caret)) install.packages("caret")
if (!require(MASS)) install.packages("MASS")
library(caret)
library(MASS)

# Identificazione e rimozione delle variabili categoriche
train_data <- train_data[, sapply(train_data, is.numeric)]
test_data <- test_data[, sapply(test_data, is.numeric)]
test_data <- na.omit(test_data)  # Rimuove tutte le righe con NA
train_data <- na.omit(train_data)
# Modello iniziale con le variabili numeriche rimanenti
fit_initial <- lm(Item_Outlet_Sales ~ ., data = train_data)

# Selezione delle variabili usando il metodo backward
step_model <- stepAIC(fit_initial, direction = "backward", trace = FALSE)

# Stampa del modello finale dopo la selezione backward
#print(summary(step_model))

# Previsione sul test set utilizzando il modello selezionato
predictions <- predict(step_model, newdata = test_data)

# Calcolo delle metriche di performance sul test set
actuals <- test_data$Item_Outlet_Sales
rmse_value <- sqrt(mean((predictions - actuals)^2))
r_squared <- summary(lm(actuals ~ predictions))$r.squared

# Stampa delle metriche di performance
cat("Performance Metrics on Test Set:\n")
cat("RMSE:", rmse_value, "\n")
cat("R-squared:", r_squared, "\n")

# Puoi anche desiderare di vedere come il modello si comporta sul set di training per confronto
train_predictions <- predict(step_model, newdata = train_data)
train_actuals <- train_data$Item_Outlet_Sales
train_rmse <- sqrt(mean((train_predictions - train_actuals)^2))
train_r_squared <- summary(lm(train_actuals ~ train_predictions))$r.squared

# Stampa delle metriche di performance sul train set
#cat("Performance Metrics on Train Set:\n")
#cat("RMSE:", train_rmse, "\n")
#cat("R-squared:", train_r_squared, "\n")
```


```{r random forest tree using only selected variables, echo=FALSE, warning=FALSE}
# Install and load the randomForest package
if (!require(randomForest)) {
    install.packages("randomForest", dependencies = TRUE)
}
library(randomForest)

# Assuming 'train_data' and 'test_data' are your datasets
# Make sure all categorical variables are converted to factors if they are not already

# Convert factors (Uncomment and adjust as necessary)
# train_data$Outlet_Type <- as.factor(train_data$Outlet_Type)
# test_data$Outlet_Type <- as.factor(test_data$Outlet_Type)
# Additional categorical variables should be handled similarly.

# Prepare data for Random Forest
train_features <- train_data[, names(train_data) != "Item_Outlet_Sales"]
train_labels <- train_data$Item_Outlet_Sales

test_features <- test_data[, names(test_data) != "Item_Outlet_Sales"]
test_labels <- test_data$Item_Outlet_Sales

# Train the Random Forest model
random_forest_model <- randomForest(x = train_features, y = train_labels, ntree = 200, mtry = 7, importance = TRUE)#mtry da 3 a 18, cambia tree

# Predictions on the test set
rf_predictions <- predict(random_forest_model, newdata = test_features)

# Calculate RMSE and R-squared
rf_rmse <- sqrt(mean((rf_predictions - test_labels)^2))
rf_r_squared <- summary(lm(test_labels ~ rf_predictions))$r.squared

# Print performance metrics
cat("Random Forest Performance Metrics on Test Set:\n")
cat("RMSE:", rf_rmse, "\n")
cat("R-squared:", rf_r_squared, "\n")

# Optionally, print variable importance
#print(importance(random_forest_model), type = 1)  # type = 1 for mean decrease in accuracy

```



```{r random forest tree mtry and node size number detection}
# Install and load necessary packages
# Install and load necessary packages
if (!require("caret")) install.packages("caret", dependencies = TRUE)
if (!require("ranger")) install.packages("ranger")
library(caret)
library(ranger)

# Load your data
df <- na.omit(df)

# Prepare train and test sets
set.seed(123)  # For reproducibility
index <- createDataPartition(df$Item_Outlet_Sales, p = 0.8, list = FALSE)
train_data <- df[index, ]
test_data <- df[-index, ]

# Define training control
train_control <- trainControl(
  method = "cv",          # Use cross-validation
  number = 10,            # Number of folds in cross-validation
  savePredictions = "final",
  verboseIter = TRUE
)

# Define the tuning grid
grid <- expand.grid(
  mtry = c(2,18, by=1),                  # Number of variables randomly sampled as candidates at each split
  splitrule = c("variance"),          # Use variance as the split rule for regression
  min.node.size = c(5, 350, by=20)            # Minimum size of terminal nodes
)

# Train the model using Random Forest via 'ranger' for regression
model <- train(
  Item_Outlet_Sales ~ .,             # Regression formula
  data = train_data,                 # Training data
  method = "ranger",                 # Use the 'ranger' package
  trControl = train_control,         # Training control
  tuneGrid = grid,                   # Grid of hyperparameters
  metric = "RMSE",                   # Optimization metric for regression
  maximize = FALSE                   # Minimize the metric (since it's RMSE)
)

# View the best tuning parameters
print(model$bestTune)

# View the results of all tuning iterations
print(model$results)

```
After we applied the Random Forest we then perform Hyperparameter Tuning, the results are positive as with node_size = 5 and mtry = 18 we were able to obtain 0.6 of $R^2$ which is a positive increase cmapring to the previous models. 

At first glance these scores could seem very low as normally a good model could reach even 0.8, but after a reasoning we conclude that this could be a great result as our dataset is about marketing techniques and some features can be interpreted in a very different way so we could expect a slightly lower final score as we're not dealing with scientific data.

#Task2

## Clustering

In this sections, we decided to perform clustering on `Item_MRP` and `Outlet_Type` to explore whether the maximum retail price of items varies significantly based on the type of outlet. By grouping the data into clusters, we aim to identify patterns and differences in pricing strategies across different outlet types. Specifically, we used K-means clustering with different numbers of clusters that we calculate later to see if certain outlet types tend to have higher or lower MRPs for their products.

### Determining optimal number of clusters using the elbow method

```{R elbow method, echo=FALSE}

# Load necessary library
if (!require("stats")) install.packages("stats")
library(stats)

#scaliamo la variabile item_MRP

df$Item_MRP <- scale(df$Item_MRP)

# Perform k-means clustering to determine the optimal number of clusters using the Elbow method
set.seed(123)
wcss <- sapply(1:10, function(k) {
  kmeans(df[, c("Item_MRP", "Outlet_Type.Grocery Store", "Outlet_Type.Supermarket Type1",
                "Outlet_Type.Supermarket Type2", "Outlet_Type.Supermarket Type3")], centers = k, nstart = 25)$tot.withinss
})

# Plotting the Elbow Curve
if (!require("ggplot2")) install.packages("ggplot2")
library(ggplot2)
elbow_plot <- data.frame(k = 1:20, wcss = wcss)
ggplot(elbow_plot, aes(x = k, y = wcss)) +
  geom_line() +
  geom_point() +
  labs(title = "Elbow Method for Determining Optimal k",
       x = "Number of Clusters k",
       y = "Total Within-Cluster Sum of Squares (WCSS)") +
  theme_minimal()
```

To find the perfect number of clusters we used the Elbow Method and the last point before the curve start increasing is 9 so the perfect number should be 9.

```{r silhouettes analysis with both 9 and 2 clusters, echo=FALSE}
# K-means clustering with k = 9
optimal_k_9 <- 9
kmeans_result_9 <- kmeans(df[, c("Item_MRP", "Outlet_Type.Grocery Store", "Outlet_Type.Supermarket Type1",
                                 "Outlet_Type.Supermarket Type2", "Outlet_Type.Supermarket Type3")],
                          centers = optimal_k_9, nstart = 50)

# K-means clustering with k = 2
optimal_k_2 <- 2
kmeans_result_2 <- kmeans(df[, c("Item_MRP", "Outlet_Type.Grocery Store", "Outlet_Type.Supermarket Type1",
                                 "Outlet_Type.Supermarket Type2", "Outlet_Type.Supermarket Type3")],
                          centers = optimal_k_2, nstart = 50)

# Adding cluster assignments to the dataframe
df$Cluster_9 <- kmeans_result_9$cluster
df$Cluster_2 <- kmeans_result_2$cluster

# Plotting the clusters

# Plot for k = 9
p1 <- ggplot(df, aes(x = factor(Cluster_9), y = Item_MRP, fill = factor(Cluster_9))) +
  geom_boxplot() +
  labs(title = "Distribution of Item MRP Across Clusters (k = 9)",
       x = "Cluster",
       y = "Item MRP") +
  scale_fill_brewer(palette = "Set3") +
  theme_minimal()

# Plot for k = 2
p2 <- ggplot(df, aes(x = factor(Cluster_2), y = Item_MRP, fill = factor(Cluster_2))) +
  geom_boxplot() +
  labs(title = "Distribution of Item MRP Across Clusters (k = 2)",
       x = "Cluster",
       y = "Item MRP") +
  scale_fill_brewer(palette = "Set3") +
  theme_minimal()

# Arranging plots in a grid
grid.arrange(p1, p2, nrow = 2)
```
After we selected nine clusters (k=9) based on the elbow method we can see frome the first plot the distribution of `Item_MRP` across these nine clusters. 
However, we decided to simplify the clustering by reducing the number of clusters to two (k=2) because we needed to simplify the analysis and interpretation. The second plot illustrates the distribution of the MRP across these two clusters, clearly showing a division into High and Low MRP categories.

```{r k-means clustering, echo=FALSE}
# Assuming the elbow point is at k = 9 based on the plot
optimal_k <- 2
kmeans_result <- kmeans(df[, c("Item_MRP", "Outlet_Type.Grocery Store", "Outlet_Type.Supermarket Type1",
                               "Outlet_Type.Supermarket Type2", "Outlet_Type.Supermarket Type3")], centers = optimal_k, nstart = 25)

# Add cluster labels to the original data
df$Cluster <- kmeans_result$cluster

# Summary by cluster
summary <- aggregate(cbind(Item_MRP) ~ Cluster, data = df, FUN = mean)
print(summary)
```
Here through this table we can see the mean values of the Item_MRP between the two clusters

```{r plotting the cluster, include=FALSE}
# Plotting the clusters
ggplot(df, aes(x = Cluster, y = Item_MRP, color = factor(Cluster))) +
  geom_jitter(alpha = 0.5) +
  facet_wrap(~ Cluster) +
  labs(title = "Cluster of Item MRP across Outlet Types",
       x = "Cluster",
       y = "Scaled Item MRP",
       color = "Cluster") +
  theme_minimal()
```

```{r item outlet sales cluster mrp, include=FALSE}
if (!require("ggplot2")) install.packages("ggplot2", dependencies = TRUE)
library(ggplot2)

# 1. Statistiche Descrittive per Item_Outlet_Sales per Cluster
sales_stats <- aggregate(Item_Outlet_Sales ~ Cluster, data = df, FUN = function(x) c(mean = mean(x), sd = sd(x), median = median(x), IQR = IQR(x)))
print(sales_stats)

# 2. Test di Significatività tra i due Cluster
# Controlla prima che entrambi i cluster abbiano più di un elemento
if (any(table(df$cluster) > 1)) {
    t_test_results <- t.test(Item_Outlet_Sales ~ cluster, data = df)
    print(t_test_results)
}

# 3. Visualizzazione con Box Plot
ggplot(df, aes(x = factor(Cluster), y = Item_Outlet_Sales, fill = factor(Cluster))) +
  geom_boxplot() +
  labs(title = "Distribution of Item Outlet Sales Across Clusters",
       x = "Cluster",
       y = "Item Outlet Sales",
       fill = "Cluster") +
  theme_minimal()

```

```{r cluster analyzing plots}
if (!require("ggplot2")) install.packages("ggplot2", dependencies = TRUE)
if (!require("gridExtra")) install.packages("gridExtra", dependencies = TRUE)
library(ggplot2)
library(gridExtra)

# Plotting the clusters
p1 <- ggplot(df, aes(x = Cluster, y = Item_MRP, color = factor(Cluster))) +
  geom_jitter(alpha = 0.5) +
  facet_wrap(~ Cluster) +
  labs(title = "Cluster of Item MRP across Outlet Types",
       x = "Cluster",
       y = "Scaled Item MRP",
       color = "Cluster") +
  theme_minimal()

# Statistiche Descrittive per Item_Outlet_Sales per Cluster
sales_stats <- aggregate(Item_Outlet_Sales ~ Cluster, data = df, FUN = function(x) c(mean = mean(x), sd = sd(x), median = median(x), IQR = IQR(x)))
print(sales_stats)

# Test di Significatività tra i due Cluster
# Controlla prima che entrambi i cluster abbiano più di un elemento
if (any(table(df$Cluster) > 1)) {
  t_test_results <- t.test(Item_Outlet_Sales ~ Cluster, data = df)
  print(t_test_results)
}

# Visualizzazione con Box Plot
p2 <- ggplot(df, aes(x = factor(Cluster), y = Item_Outlet_Sales, fill = factor(Cluster))) +
  geom_boxplot() +
  labs(title = "Distribution of Item Outlet Sales Across Clusters",
       x = "Cluster",
       y = "Item Outlet Sales",
       fill = "Cluster") +
  theme_minimal()

# Arrange the plots in a grid
grid.arrange(p1, p2, ncol = 2)

```
After that we aimed to verify if certain products were concentrated in one of the two clusters but more importantly we would like to see if the products that have more sales are the ones with an higher MRP and from the second plot we can clearly see that this stands true confirming what we already saw in our EDA where higher MRP brings higher numbers of sales.

```{r outlet type vs cluster mrp, echo=FALSE, message=FALSE, warning=FALSE}
# Rename columns to remove spaces
names(df) <- gsub(" ", "", names(df))

# Check updated column names
print(names(df))

# Recreate the Outlet_Type variable from one-hot encoding
df$Outlet_Type <- ifelse(df$Outlet_Type.GroceryStore == 1, "Grocery Store",
                         ifelse(df$Outlet_Type.SupermarketType1 == 1, "Supermarket Type1",
                                ifelse(df$Outlet_Type.SupermarketType2 == 1, "Supermarket Type2",
                                       ifelse(df$Outlet_Type.SupermarketType3 == 1, "Supermarket Type3", NA))))

# Check the new variable creation
table(df$Outlet_Type)

# Create a contingency table between price clusters and outlet types
outlet_price_cluster_table <- table(df$Outlet_Type, df$Cluster)
print(outlet_price_cluster_table)

# Convert the table to a long format for ggplot2
df_long <- reshape2::melt(outlet_price_cluster_table)

# Barplot using ggplot2 (Base R equivalent)
barplot1 <- ggplot(df_long, aes(x = Var1, y = value, fill = Var2)) +
            geom_bar(stat = "identity", position = position_dodge()) +
            labs(x = "Outlet Type", y = "Count", fill = "Cluster") +
            ggtitle("Distribution of Types Across Price Clusters") +
            theme_minimal()

# Convert the table to a grob for displaying
table_grob <- tableGrob(outlet_price_cluster_table)

# Combine the table and the bar plot in a grid
grid.arrange(table_grob, barplot1, ncol = 2)
```
We wanted to see which types of outlets fall into high and low price clusters. Based on economic reasoning, we expected that Grocery Stores and Supermarket Type 1 would fall into the high-price cluster due to their limited product variety, implying less internal competition. However, our analysis revealed a more uniform distribution of outlet types between the price clusters. This suggests that no particular outlet type dominates a specific price cluster, indicating similar pricing strategies or product ranges across different outlet types. Essentially, price segmentation is not heavily influenced by the outlet type. You can see this clearly in the table and the bar plot, where the counts of each outlet type are relatively balanced across the clusters. This outcome was a bit surprising but provides valuable insight into how BigMart’s outlets manage their pricing strategies regardless of the outlet type.

UN ULTIMA ANALISI CHE FACCIAMO E' TRA ITEM MRP E PRODOTTI GRASSI/NON GRASSI.


```{R cluster on item fat content}
# Crea sottoinsiemi per Regular e Low Fat considerando la codifica numerica
df_regular <- df[df$Item_Fat == 2, ]  # Regular Fat
df_low <- df[df$Item_Fat == 1, ]      # Low Fat

# Statistiche per prodotti Regular e Low Fat in ciascun cluster
regular_stats <- aggregate(Item_MRP ~ Cluster, data = df_regular, FUN = mean)
low_fat_stats <- aggregate(Item_MRP ~ Cluster, data = df_low, FUN = mean)

print(regular_stats)
print(low_fat_stats)

# Test t per confrontare i prezzi tra Regular Fat (2) e Low Fat (1) in ciascun cluster
t_test_cluster1 <- t.test(Item_MRP ~ Item_Fat_Content, data = df[df$Cluster == '1', ], subset = Item_Fat_Content %in% c(1, 2))
t_test_cluster2 <- t.test(Item_MRP ~ Item_Fat_Content, data = df[df$Cluster == '2', ], subset = Item_Fat_Content %in% c(1, 2))

print(t_test_cluster1)
print(t_test_cluster2)

# Visualizzazione con ggplot2
library(ggplot2)
ggplot(df, aes(x = factor(Item_Fat_Content), y = Item_MRP, fill = factor(Item_Fat_Content))) +
  geom_boxplot() +
  facet_wrap(~Cluster) +
  scale_fill_manual(values = c("blue", "red"), labels = c("Low Fat", "Regular Fat")) +
  labs(title = "Distribution of Item MRP by Item Fat Content Across Clusters",
       x = "Item Fat Content",
       y = "Item Maximum Retail Price (MRP)",
       fill = "Item Fat") +
  theme_minimal()

```


scrivi interpret di questo