---
title: "main"
output: pdf_document
date: "2024-06-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(Metrics)
library(glmnet)
library(mgcv)
library(rpart)
library(e1071)
library(splines)
library(MASS)
```

Importing dataset and libraries
```{r df_import, include=FALSE}
a="/Users/alessandroausteri/Documents/GitHub/Data_Analysis_Final/BigMartSales.csv"
l="/Users/lorenzolaterza/Desktop/Data_Analysis_Final/BigMartSales.csv"


df= read.table(a,header=TRUE, sep=",") 
library(dplyr)
library(corrplot)
library(ggplot2)
library(gridExtra)
```
#EDA
##Data Cleaning

###Checking and Removing Duplicates
```{r Removing Duplicates}
# Check for entire row duplicates
duplicates_entire_row <- duplicated(df)

# Count the number of entirely duplicated rows
num_entire_row_duplicates <- sum(duplicates_entire_row)
print(paste("Number of duplicated rows:", num_entire_row_duplicates))
```
As we can see there aren’t any duplicates in our data set, so we can proceed to find which columns have missing values

###Fixing Features of the Dataset

 
```{r unique values}
item_fat_content_uniques <- unique(df$Item_Fat_Content)#unique values of item_fat_content

print(item_fat_content_uniques)
```

```{r sistemare nomi item_fat_content}
df$Item_Fat_Content <- ifelse(grepl("^[Rr]", df$Item_Fat_Content), "Regular Fat", "Low Fat")
item_fat_content_uniques <- unique(df$Item_Fat_Content)

print(item_fat_content_uniques)

```
If we go throughout the column `Item_Fat_Content` we can cearly see that there are some inconsistencies, in fact we have 2 different names for "Regular Fat" that are "Regular" and "reg" and 3 different names for "Low Fat" that are "Low fat", "low fat", "LF"

To solve this problem, we edited the column "Item_Fat_Content" in this way:
all records that start with R or r, will be renamed "Regular Fat", and other will be renamed "Low Fat".

Then we will print the uniques name of the column after the edit to chek if the process ended successfully 


###Checking for Missing Values

```{r check for missing values}

# Crea una copia del dataframe convertendo tutte le colonne in stringhe
df_copy <- lapply(df, as.character)

# Creiamo una funzione per contare "NA", "" o "0" in ogni colonna della copia
count_na_empty_or_zero_strings <- function(column) {
  # Sostituiamo i veri NA con stringhe vuote per una conta coerente
  column[is.na(column)] <- ""
  sum(column == "" | column == "0", na.rm = TRUE)
}

# Applicare la funzione a tutte le colonne della copia e stampare i risultati
na_empty_or_zero_counts <- sapply(df_copy, count_na_empty_or_zero_strings)

# Stampa il numero di "NA", "" o "0" per ogni colonna della copia
print(na_empty_or_zero_counts)
```
We see that there are 3 columns with missing values:
-Item_Weight
-Item_Visibility
-Outlet_Size


```{r Outlet size analysis}
#conteggio records senza Outlet_Size:
null<-sum(nchar(df$Outlet_Size) ==0)

#numero di records
total<- nrow(df)

print(paste("Number of rows with missing valuess on Outlet_Size:",null))

#percentuale di righe senza outlet_size
percentuale_stringhe_vuote <- (null / total) * 100

#stampa percentuale di righe senza outlet_size

print(paste("Percentuale di records con stringhe vuote in 'Outlet_Size':", percentuale_stringhe_vuote, "%"))
```






```{r updating outlet_size}
# Aggiornamento della colonna 'Outlet_Size'
df <- df %>%
  mutate(Outlet_Size = case_when(
    Outlet_Type == "Grocery Store" ~ "Small",
    #Outlet_Type %in% c("Supermarket Type2", "Supermarket Type3") ~ "Medium",
    TRUE ~ Outlet_Size  # Mantiene il valore originale per tutte le altre condizioni
  ))

# Visualizza le modifiche per confermare
# Creazione di una tabella di riepilogo
outlet_summary <- df %>%
  filter(Outlet_Size %in% c("Small", "Medium", "High")) %>%  # Filtra per includere solo le righe con i valori specificati
  group_by(Outlet_Type, Outlet_Size) %>%  # Raggruppa per tipo e dimensione del negozio
  summarise(Count = n(), .groups = 'drop')  # Calcola il conteggio e rimuove il raggruppamento automatico

# Visualizzazione della tabella di riepilogo
print(outlet_summary)

```

```{r Outlet_Size Analysis 2}
#conteggio records senza Outlet_Size:
null<-sum(nchar(df$Outlet_Size) ==0)

#numero di records
total<- nrow(df)

print(paste("Number of rows with missing valuess on Outlet_Size:",null))

#percentuale di righe senza outlet_size
percentuale_stringhe_vuote <- (null / total) * 100

#stampa percentuale di righe senza outlet_size

print(paste("Percentuale di records con stringhe vuote in 'Outlet_Size':", percentuale_stringhe_vuote, "%"))
```




```{r adding na values}
# Aggiornamento della colonna 'Outlet_Size' per riempire le stringhe vuote
df <- df %>%
  mutate(Outlet_Size = if_else(nchar(Outlet_Size) == 0, "NA", Outlet_Size))
```
To address the significant number of missing values were noted in the `Outlet_Size` column, we first calculated the percentage of missing entries to understand the scope of the issue.

Subsequently, we explored potential relationships between `Outlet_Type` and `Outlet_Size` by creating a cross-tabulation of these variables. This analysis revealed distinct patterns: all entries categorized as "Grocery Store" were consistently labeled as "Small", while "Supermarket Type2" and "Supermarket Type3" were uniformly classified as "Medium".

Considering this insight, we proceeded to change missing `Outlet_Size` values based on `Outlet_Type`:

For "Grocery Store", missing sizes were filled with "Small".
For "Supermarket Type2" and "Supermarket Type3", missing sizes were filled with "Medium".
These imputations reduced the percentage of missing values from 28% to 21%, decreasing the total number of missing entries from 2410 to 1855.

Regarding "Supermarket Type1" we do not have enough informations or clear patterns, so we had to fill the remaining missing entries with a placeholder value of "NA", ensuring no data point within "Outlet_Size" remained unaddressed.


```{r factor converting}

# Conversione di 'Outlet_Size' in valori numerici
df <- df %>%
  mutate(Outlet_Size = case_when(
    Outlet_Size == "Small" ~ 1,
    Outlet_Size == "Medium" ~ 2,
    Outlet_Size == "High" ~ 3,
    TRUE ~ NA_real_  # Imposta NA per qualsiasi altro valore non specificato
  ))

df <- df%>%
  mutate(Item_Fat_Content = case_when(
    Item_Fat_Content == "Low Fat" ~ 1,
    Item_Fat_Content == "Regular Fat" ~ 2,
    TRUE ~ NA_real_
  ))
head(df)
```
We converted as factor the columns `Item_Fat_Content` and we also want to convert the column `Outlet_Size` giving respectively:
0 -> "Low Fat" 
1 -> "Regular"

1 -> Small
2 -> Medium
3 -> Large




```{r handling missing item_weight}

# Calcolare la media del peso per ogni categoria di prodotto
average_weight_per_type <- aggregate(Item_Weight ~ Item_Type, data = df, mean, na.rm = TRUE)

# Funzione per riempire i pesi mancanti
fill_missing_weights <- function(item_id, item_type) {
  # Controlla se esiste un valore non NA per lo stesso Item_Identifier
  if (any(!is.na(df$Item_Weight[df$Item_Identifier == item_id]))) {
    return(df$Item_Weight[df$Item_Identifier == item_id & !is.na(df$Item_Weight)][1])
  } else {
    # Altrimenti usa la media del Item_Type corrispondente
    return(average_weight_per_type$Item_Weight[average_weight_per_type$Item_Type == item_type])
  }
}

# Applicare la funzione ai valori NA in Item_Weight
df$Item_Weight[is.na(df$Item_Weight)] <- mapply(fill_missing_weights, df$Item_Identifier[is.na(df$Item_Weight)], df$Item_Type[is.na(df$Item_Weight)])

# Visualizzare il dataframe aggiornato
head(df)


```
Here we handled missing values for `Item_Weight`. 
The first idea was to use the general mean of `Item_Weight`, but since we have also the category of each product, we can compute the mean of every product category.
At the end, we checked for every record with missing `Item_Weight` and we will do a double check:
1. If there were another item with the same id and missing weight, they will have for sure the same weight.
2. We will put in item_weight, the mean of the weights of the products of the same category.


```{r count 0 values}
# Find the number of zero 'Item_Visibility' values for each 'Outlet_Identifier'
zero_visibility_counts <- aggregate(Item_Visibility ~ Outlet_Identifier, data = df, function(x) sum(x == 0))

# Rename the column for better understanding
names(zero_visibility_counts)[2] <- "Zero_Item_Visibility_Count"

# Display the result
print(zero_visibility_counts)
```

```{r count Visibility for each store 2}

visibility_sum_per_store <- aggregate(Item_Visibility ~ Outlet_Identifier, data = df, sum)
names(visibility_sum_per_store)[2] <- "Total_Item_Visibility"

# Display the result
print(visibility_sum_per_store)
```

```{r Visibility}

# Filtrare i record con visibilità maggiore di zero per calcolare le medie
filtered_df <- df[df$Item_Visibility > 0, ]

# Calcolare la media di Item_Visibility per ogni combinazione di Item_Type e Outlet_Size
visibility_avg_per_type_size <- aggregate(Item_Visibility ~ Item_Type + Outlet_Size, data = filtered_df, mean)

# Creare una chiave unica per facilitare il merge
visibility_avg_per_type_size$key <- with(visibility_avg_per_type_size, paste(Item_Type, Outlet_Size, sep = "_"))
df$key <- with(df, paste(Item_Type, Outlet_Size, sep = "_"))

# Merge tra i record del dataframe originale e le medie calcolate usando un left join
df <- merge(df, visibility_avg_per_type_size, by = "key", all.x = TRUE, suffixes = c("", ".new"))

# Sostituire i valori zero di Item_Visibility con i valori medi calcolati
df$Item_Visibility[df$Item_Visibility == 0] <- df$Item_Visibility.new[df$Item_Visibility == 0]

# Rimuovere le colonne in più create dal merge
df <- df[, !names(df) %in% c("Item_Visibility.new", "key")]

# Visualizzare il dataframe aggiornato
head(df)
```
```{r dropping useless }
df <- df %>%
  select(-Item_Type.new, -Outlet_Size.new)
```
Finally we fixed the missing values for `Item_Visibility`. To do it we firstly count the number of missing values for each of the 10 shops and their visibility to understand how much visibility we are missing from each store. Our first idea was to subtract each visibility percentage from 100% and then equally divide the remaining visibility among the number of missing visibility product. In doing that we obtained biased and not consistent data because:

1. The filled visibility in each product was too high compared to the non-zeros values
2. After adding again all the visibility the result was 100% for each shop, but could also be true that in some shops some products are out of stock so the final visibility could be less than 100%

For this reason we decided to change our strategy and to fill the missing values we firstly find the missing `Item_Visibility`, then through the `Item_ID` we find all the shops that have for that product a real value, then we check the `Outlet_Size` of each shop and compare to the one of the market with missing value if it's the same we compute the mean throu all the visibility that match these criteria.

We did this because for example if the missing visibility is in a Medium shop, we cannot consider the same product visibility in a Small shop, because it's obviously higher, so we take into accounts a visibility to compute the mean only if the `Outlet_Size` is the same.


```{r unique categories of "Item_Type"}
#finding out all the uniques


item_fat_content_uniques <- unique(df$Item_Type)#unique values of item_fat_content

print(item_fat_content_uniques)
```

```{r categories reduction of "Item_Type"}
df <- df %>%
  mutate(Item_Type_Reduced = case_when(
    Item_Type %in% c("Baking Goods", "Breads", "Breakfast", "Dairy", "Meat", "Seafood") ~ "Food Basics",
    Item_Type %in% c("Canned", "Frozen Foods", "Snack Foods", "Starchy Foods") ~ "Processed Foods",
    Item_Type %in% c("Hard Drinks", "Soft Drinks") ~ "Beverages",
    Item_Type %in% c("Health and Hygiene", "Household") ~ "Non-Food Items",
    Item_Type == "Fruits and Vegetables" ~ "Produce",
    Item_Type == "Others" ~ "Others",
    TRUE ~ "Others"  # Catch-all for any undefined categories
  ))

#rename the column item_type_reduced into item_type

#drop old item_Type column

df <- df %>%
  select(-Item_Type)


# Renaming the column using dplyr
df <- df %>%
  rename(Item_Type = Item_Type_Reduced)
```




```{r visibility plot}
# Histogram and Box Plot for Item_Visibility
p3 <- ggplot(df, aes(x = Item_Visibility)) + 
  geom_histogram(binwidth = 0.01, fill = 'green', alpha = 0.7) + 
  ggtitle("Distribution of Item Visibility") +
  xlab("Item Visibility") +
  ylab("Frequency")

p4 <- ggplot(df, aes(x = "", y = Item_Visibility)) + 
  geom_boxplot(fill = 'green', alpha = 0.7) + 
  ggtitle("Box Plot of Item Visibility") +
  xlab("") +
  ylab("Item Visibility")
print(p3)
print(p4)
```
Finally we fixed the missing values for `Item_Visibility`. To do it we firstly count the number of missing values for each of the 10 shops and their visibility to understand how much visibility we are missing from each store. Our first idea was to subtract each visibility percentage from 100% and then equally divide the remaining visibility among the number of missing visibility product. In doing that we obtained biased and not consistent data because:

1. The filled visibility in each product was too high compared to the non-zeros values
2. After adding again all the visibility the result was 100% for each shop, but could also be true that in some shops some products are out of stock so the final visibility could be less than 100%

For this reason we decided to change our strategy and to fill the missing values we firstly find the missing `Item_Visibility`, then through the `Item_ID` we find all the shops that have for that product a real value, then we check the `Outlet_Size` of each shop and compare to the one of the market with missing value if it's the same we compute the mean throu all the visibility that match these criteria.

We did this because for example if the missing visibility is in a Medium shop, we cannot consider the same product visibility in a Small shop, because it's obviously higher, so we take into accounts a visibility to compute the mean only if the `Outlet_Size` is the same.




```{r count Visibility for each store}

visibility_sum_per_store <- aggregate(Item_Visibility ~ Outlet_Identifier, data = df, sum)
names(visibility_sum_per_store)[2] <- "Total_Item_Visibility"

# Display the result
print(visibility_sum_per_store)
```


##Multivariate Analysis

Now we will start to find out some correlations between variables.

```{R corr matrix}

# Calculate Spearman's rank correlation matrix again if not already calculated
cor_matrix <- cor(df %>% select(where(is.numeric)), 
                  method = "spearman", 
                  use = "pairwise.complete.obs")

# Visualize the correlation matrix with coefficients inside the circles
corrplot(cor_matrix, method = "circle", type = "upper", order = "hclust",
         tl.col = "black", tl.srt = 45, tl.cex = 0.8, cl.cex = 0.8,
         col = colorRampPalette(c("#6BAED6", "#FFFFFF", "#FD8D3C"))(200),
         addCoef.col = "black",  # Sets color of the coefficients to black (choose based on your color scheme)
         number.cex = 0.6)  # Adjust coefficient text size appropriately

```
DOBBIAMO DROPPARE LE COLONNE REMINAINING VISIBILITY EZERO ITEM VISIBILITY COUNT



We can see that there are big positive relation between:

outlet_size-item visibility
item_outlet_sales and outlet_size
item_mrp and item_outlet_sales

```{r item MRP vs Outlet sales with gaps of price }
# Load necessary libraries
library(ggplot2)

# Specific price points to add to the plot
specific_price_points <- c(67.99, 134.49, 199.99)

# Plot the relationship between Item_MRP and Item_Outlet_Sales
p <- ggplot(df, aes(x = Item_MRP, y = Item_Outlet_Sales)) +
  geom_point(alpha = 0.5) +
  labs(title = "Relationship between Item MRP and Outlet Sales",
       x = "Item MRP", y = "Outlet Sales")

# Add vertical lines for specific price points
for (price in specific_price_points) {
  p <- p + geom_vline(xintercept = price, linetype = "dotted", color = "blue", alpha = 0.7)
  p <- p + annotate("text", x = price, y = max(df$Item_Outlet_Sales) * 0.9, label = price, color = "blue", angle = 90, vjust = -0.5, size = 3)
}

# Print the plot
print(p)

```
```{R Relationship between Item MRP and Outlet Sales by Product Type}

library(dplyr)

df_summary <- df %>%
  group_by(Item_Type, MRP_Bracket = cut(Item_MRP, breaks = seq(0, max(Item_MRP), by = 20))) %>%
  summarize(Average_Sales = mean(Item_Outlet_Sales), .groups = 'drop')

ggplot(data = df_summary, aes(x = MRP_Bracket, y = Average_Sales, fill = Item_Type)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Average Outlet Sales by MRP Bracket and Product Type",
       x = "MRP Bracket",
       y = "Average Outlet Sales") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

zi qua c'è scritto che più vendi e più alzi il prezzo di base


```{R item outlet sales by outlet size}
ggplot(df, aes(x = Outlet_Size, y = Item_Outlet_Sales)) +
  geom_boxplot(aes(fill = Outlet_Size)) +
  labs(title = "Item Outlet Sales by Outlet Size",
       x = "Outlet Size", y = "Outlet Sales")
```

```{R Sales trends over the years}
ggplot(df, aes(x = Outlet_Establishment_Year, y = Item_Outlet_Sales)) +
  geom_point(aes(color = Outlet_Establishment_Year)) +
  geom_smooth(method = "lm") +
  labs(title = "Sales Trends Over the Years",
       x = "Establishment Year", y = "Outlet Sales")

```

```{R  Item_Outlet_Sales density plot}
# Create a density plot
ggplot(data = df, aes(x = Item_Outlet_Sales)) +
  geom_density(fill = "turquoise", alpha = 0.5) +  # 'alpha' controls transparency
  labs(title = "Density Plot of Item Outlet Sales",
       x = "Item Outlet Sales",
       y = "Density") +
  theme_light()
```

```{r Advanced visualization Visibility-Sales}
# Ensure 'Item_Visibility' and 'Outlet_Sales' are treated correctly
df$Item_Visibility <- as.numeric(as.character(df$Item_Visibility))
df$Item_Outlet_Sales <- as.numeric(as.character(df$Item_Outlet_Sales))

# Convert categorical variables to factors
df$Item_Type <- as.factor(df$Item_Type)
df$Outlet_Type <- as.factor(df$Outlet_Type)

# Segmented Regression by Item Type
item_type_models <- df %>%
  group_by(Item_Type) %>%
  do(model = lm(Item_Outlet_Sales ~ Item_Visibility, data = .))

# Viewing summaries for each Item Type
item_type_summaries <- lapply(item_type_models$model, summary)

# Print the summaries for review
print(item_type_summaries)

# Creating scatter plots segmented by Item_Type
p_item_type <- ggplot(df, aes(x = Item_Visibility, y = Item_Outlet_Sales)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", color = "blue", se = FALSE) +
  facet_wrap(~ Item_Type) +
  labs(title = "Item Visibility vs Outlet Sales by Item Type",
       x = "Item Visibility", y = "Outlet Sales") +
  theme_minimal()

# Creating scatter plots segmented by Outlet_Type
p_outlet_type <- ggplot(df, aes(x = Item_Visibility, y = Item_Outlet_Sales)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  facet_wrap(~ Outlet_Type) +
  labs(title = "Item Visibility vs Outlet Sales by Outlet Type",
       x = "Item Visibility", y = "Outlet Sales") +
  theme_minimal()

# Print the plots
print(p_item_type)
print(p_outlet_type)
```

```{r Linear model to investigate on Item Visibility and Sales}
# Fit a linear model
model <- lm(Item_Outlet_Sales ~ Item_Visibility + Item_Type + Outlet_Type + Item_Fat_Content + Item_MRP, data = df)

# Summary of the model to understand influences
summary(model)
```

```{r Distribution of Variables across Item Sales}
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Convert categorical variables to factor if not already
df$Item_Type <- as.factor(df$Item_Type)
df$Outlet_Type <- as.factor(df$Outlet_Type)
df$Item_Fat_Content <- as.factor(df$Item_Fat_Content)
df$Outlet_Size <- as.factor(df$Outlet_Size)
df$Outlet_Location_Type <- as.factor(df$Outlet_Location_Type)

# Continuous Variables
continuous_vars <- c("Item_Weight", "Item_Visibility", "Item_MRP")

# Categorical Variables
categorical_vars <- c("Item_Type", "Outlet_Type", "Item_Fat_Content", "Outlet_Size", "Outlet_Location_Type")

# Plotting Distribution of Sales for Continuous Variables
for(var in continuous_vars) {
  p <- ggplot(df, aes_string(x = var, y = "Item_Outlet_Sales")) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "lm", color = "blue") +
    labs(title = paste("Scatter Plot of", var, "vs Outlet Sales"),
         x = var, y = "Outlet Sales") +
    theme_minimal()
  print(p)
}

# Plotting Distribution of Sales for Categorical Variables
for(var in categorical_vars) {
  p <- df %>%
    group_by_(.dots = var) %>%
    summarise(Average_Sales = mean(Item_Outlet_Sales)) %>%
    ggplot(aes_string(x = var, y = "Average_Sales", fill = var)) +
    geom_bar(stat = "identity", color = "black") +
    labs(title = paste("Average Sales by", var),
         x = var, y = "Average Sales") +
    theme_minimal()
  print(p)
}

```


```{R item visibility vs item mrp scatterplot}
# Load ggplot2
library(ggplot2)


# Create a scatter plot
ggplot(df, aes(x = Item_MRP, y = Item_Visibility)) +
  geom_point(alpha = 0.5) +  # Use semi-transparent points to handle overplotting
  labs(x = "Item Maximum Retail Price (MRP)",
       y = "Item Visibility",
       title = "Relationship between Item MRP and Item Visibility") +
  theme_minimal()  # Clean minimalistic theme
```






```{R item visibility vs item mrp line graph}
# Create bins for Item MRP using a reasonable interval
df$MRP_Bin <- cut(df$Item_MRP, breaks=seq(from=min(df$Item_MRP), to=max(df$Item_MRP), by=20), include.lowest=TRUE, right=TRUE)

# Calculate average visibility per MRP bin
average_visibility_per_mrp <- aggregate(Item_Visibility ~ MRP_Bin, data = df, mean)

# Create a line graph
ggplot(average_visibility_per_mrp, aes(x = MRP_Bin, y = Item_Visibility, group=1)) +
  geom_line() +  # Adds a line graph
  geom_point() +  # Adds points to each average point
  labs(x = "Item MRP Range", y = "Average Item Visibility",
       title = "Average Item Visibility Across Different MRP Ranges") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Improve x-axis label readability
```

gli item che fanno vedere di può sono quelli che vendono a di meno quindi quelli di cui vogliono sbarazzarsi.

```{r Segmented Analysis Regression}
# Load necessary libraries
library(ggplot2)
library(dplyr)




# Segmented Regression by Outlet Type
outlet_type_models <- df %>%
  group_by(Outlet_Type) %>%
  do(model = lm(Item_Outlet_Sales ~ Item_Visibility, data = .))

# Viewing summaries for each Outlet Type
outlet_type_summaries <- lapply(outlet_type_models$model, summary)

# Print the summaries for review
print(outlet_type_summaries)

# Optional: Plotting the regression lines for each type on a scatter plot
# Plotting for Item Type
ggplot(df, aes(x = Item_Visibility, y = Item_Outlet_Sales, color = Item_Type)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~ Item_Type) +
  labs(title = "Item Visibility vs Outlet Sales by Item Type", x = "Item Visibility", y = "Outlet Sales")

# Plotting for Outlet Type
ggplot(df, aes(x = Item_Visibility, y = Item_Outlet_Sales, color = Outlet_Type)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~ Outlet_Type) +
  labs(title = "Item Visibility vs Outlet Sales by Outlet Type", x = "Item Visibility", y = "Outlet Sales")
```

Nei grocery store l'item visibility non influisce sul numero di vendite perchè gia sai cosa vai a prendere

###Checking for Outliers

```{r plots to see outliers}
# Load necessary libraries
library(ggplot2)
library(gridExtra)

# Histogram and Box Plot for Item_Weight
p1 <- ggplot(df, aes(x = Item_Weight)) + 
  geom_histogram(binwidth = 1, fill = 'blue', alpha = 0.7) + 
  ggtitle("Distribution of Item Weight") +
  xlab("Item Weight") +
  ylab("Frequency")

p2 <- ggplot(df, aes(x = "", y = Item_Weight)) + 
  geom_boxplot(fill = 'blue', alpha = 0.7) + 
  ggtitle("Box Plot of Item Weight") +
  xlab("") +
  ylab("Item Weight")

# Histogram and Box Plot for Item_Visibility
p3 <- ggplot(df, aes(x = Item_Visibility)) + 
  geom_histogram(binwidth = 0.01, fill = 'green', alpha = 0.7) + 
  ggtitle("Distribution of Item Visibility") +
  xlab("Item Visibility") +
  ylab("Frequency")

p4 <- ggplot(df, aes(x = "", y = Item_Visibility)) + 
  geom_boxplot(fill = 'green', alpha = 0.7) + 
  ggtitle("Box Plot of Item Visibility") +
  xlab("") +
  ylab("Item Visibility")

# Histogram and Box Plot for Item_MRP
p5 <- ggplot(df, aes(x = Item_MRP)) + 
  geom_histogram(binwidth = 5, fill = 'red', alpha = 0.7) + 
  ggtitle("Distribution of Item MRP") +
  xlab("Item MRP") +
  ylab("Frequency")

p6 <- ggplot(df, aes(x = "", y = Item_MRP)) + 
  geom_boxplot(fill = 'red', alpha = 0.7) + 
  ggtitle("Box Plot of Item MRP") +
  xlab("") +
  ylab("Item MRP")

# Histogram and Box Plot for Item_Outlet_Sales
p7 <- ggplot(df, aes(x = Item_Outlet_Sales)) + 
  geom_histogram(binwidth = 100, fill = 'purple', alpha = 0.7) + 
  ggtitle("Distribution of Item Outlet Sales") +
  xlab("Item Outlet Sales") +
  ylab("Frequency")

p8 <- ggplot(df, aes(x = "", y = Item_Outlet_Sales)) + 
  geom_boxplot(fill = 'purple', alpha = 0.7) + 
  ggtitle("Box Plot of Item Outlet Sales") +
  xlab("") +
  ylab("Item Outlet Sales")

# Print the plots individually
print(p1)
print(p2)
print(p3)
print(p4)
print(p5)
print(p6)
print(p7)
print(p8)

```


```{r Item type vs Item_Sales}

# Esegui ANOVA
avg_sales_by_type <- df %>%
  group_by(Item_Type) %>%
  summarise(Average_Sales = mean(Item_Outlet_Sales, na.rm = TRUE))

# Creating a bar graph of average sales by item type
ggplot(avg_sales_by_type, aes(x = Item_Type, y = Average_Sales, fill = Item_Type)) +
  geom_bar(stat = "identity", width = 0.7) +  # Using identity to use the heights of the bars to represent values in the data
  labs(title = "Average Sales by Item Type", x = "Item Type", y = "Average Sales") +
  theme_minimal() +  # Clean minimalistic theme
  theme(axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x labels for better visibility
        legend.title = element_blank()) +  # Remove legend title if not needed
  scale_fill_brewer(palette = "Paired")  # Optional: Use a color palette that is visually appealing

```
There are not effective evidences of difference in item_sales based on item_type






# Lower Dimensional Models
```{r Impact of item MRP ad Outlet Type on Sales 2 }
model_mrp_outlet <- lm(Item_Outlet_Sales ~ Item_MRP * Outlet_Type, data = df)
summary(model_mrp_outlet)

# Plotting this relationship
ggplot(df, aes(x = Item_MRP, y = Item_Outlet_Sales, color = Outlet_Type)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~Outlet_Type) +
  labs(title = "Impact of MRP and Outlet Type on Sales")
```


## Visualization Interpretation

The scatter plots across different outlet types (Grocery Store, Supermarket Type1, Type2, and Type3) show how Item_MRP influences sales:

Grocery Store: The sales seem mostly flat regardless of the MRP, suggesting that price changes do not significantly influence sales in grocery stores. This could indicate price sensitivity or a limited range of products where price does not play a major role.

Supermarket Type1: There appears to be a moderate positive relationship between MRP and sales. This indicates that as the price of an item increases, sales also tend to increase, possibly due to a perception of higher quality or the availability of a wider range of products.

Supermarket Type2: Similar to Type1, but the relationship looks slightly weaker. This might be due to different customer demographics or store locations.

Supermarket Type3: Shows a strong positive relationship between MRP and sales, suggesting that customers at Type3 supermarkets are less price-sensitive and possibly more driven by product quality or brand.

Model Summary Interpretation
The regression output provides the following insights:

Intercept: The intercept is not meaningful on its own without context as it represents sales when MRP is zero and not belonging to any specific outlet type, which is not a practical scenario.

Item_MRP: There's a significant positive coefficient for MRP, indicating that overall, an increase in MRP tends to lead to an increase in sales across the dataset.

Outlet_Type Coefficients:

Supermarket Type1: The interaction term Item_MRP:Outlet_TypeSupermarket Type1 is significant and positive, which supports the visual interpretation that higher MRP increases sales more in this supermarket type compared to the baseline (Grocery Store).
Supermarket Type2: Similarly, the positive coefficient for the interaction with MRP suggests increased sales with higher MRP, but the effect is smaller than in Type1.
Supermarket Type3: Exhibits the strongest positive interaction effect with MRP, indicating that sales in these stores are most positively influenced by higher MRP.
Model Fit:

The Multiple R-squared value is 0.6054, meaning that about 60.54% of the variability in sales is explained by the model, which is reasonably good given the complexity of retail sales dynamics.
The F-statistic is highly significant (p < 2.2e-16), indicating that the model is statistically significant and that the model fits the data better than a model with no predictors.
Conclusions
This analysis suggests that MRP and outlet type are significant predictors of sales. Higher MRP generally leads to higher sales, especially in supermarket types where customers might be less sensitive to price increases. Grocery stores show minimal changes in sales with changes in MRP, indicating a different customer base or purchase behavior.

These insights can help BigMart tailor its pricing strategies according to the type of outlet, potentially focusing on premium pricing strategies in supermarket types where the customer base responds positively to higher-priced items.





```{r Impact of item MRP ad Outlet Type on Sales }
# Adjusting the model to focus only on Item Visibility
model_visibility <- lm(Item_Outlet_Sales ~ Item_Visibility, data = df)
summary(model_visibility)
# Plotting the relationship between Item Visibility and Sales
ggplot(df, aes(x = Item_Visibility, y = Item_Outlet_Sales)) +
  geom_point(alpha = 0.3) +  # Point opacity reduced for better visualization of density
  geom_smooth(method = "lm", se = FALSE) +  # Linear model smoothing without confidence intervals
  labs(title = "Impact of Item Visibility on Sales",  # Updated title
       x = "Item Visibility",  # X-axis label
       y = "Sales")  # Y-axis label

```

Dobbiamo scriveree interprertazione di questo grafico



#Feature Engineering
```{r feature engineering}
df <- df %>%
  mutate(
    # Convert Establishment Year to Age
    Outlet_Age = as.numeric(format(Sys.Date(), "%Y")) - Outlet_Establishment_Year,
    
    # Interaction between MRP and Outlet Type
    MRP_x_OutletType = Item_MRP * as.numeric(as.factor(Outlet_Type)),

    
    # Simplifying Item Type
    Item_Type_Simplified = case_when(
      grepl("Foods", Item_Type) ~ "Foods",
      grepl("Drinks", Item_Type) ~ "Drinks",
      TRUE ~ "Non-Consumables"
    )
  )

# Ensure that categorical variables are in the correct format
df$Outlet_Type <- as.factor(df$Outlet_Type)
df$Item_Fat_Content <- as.factor(df$Item_Fat_Content)
df$Item_Type_Simplified <- as.factor(df$Item_Type_Simplified)
df$Outlet_Identifier <- as.factor(df$Outlet_Identifier)

df <- df %>%
  select(-c(Outlet_Age, MRP_x_OutletType))
```


trasformare da anno di nascita ad età

trasformare in percentuale item visibility.


Modelli di previsione su:

  -item rmp
  -outlet size(?) o type
  -Percentage of visibility
  -Outlet id
  
  
poi una volta fatto questo, scegliamo quali variabili sono più influenti e le buttiamo nel modello finale

e poi ragioniamo su come fare il modello finale




Feature Selection with Stepwise Regression



```{r calculating Outlet Age}
# Add a new column 'Outlet_Age' to the dataframe
df <- df %>%
  mutate(Outlet_Age = as.numeric(format(Sys.Date(), "%Y")) - Outlet_Establishment_Year)

# Display the age of each store
# This creates a summary table with Outlet_ID and its corresponding Outlet_Age
store_ages <- df %>%
  select(Outlet_Identifier, Outlet_Age) %>%  # Select the necessary columns
  distinct() %>%  # Remove duplicate rows to ensure each store is listed once
  arrange(Outlet_Identifier)  # Optional: Sort by Outlet Identifier for easier reading

# Print the resulting table to see each store's age
#print(store_ages)

# Aggregate total sales by Outlet_ID
sales_summary <- df %>%
  group_by(Outlet_Identifier) %>%
  summarise(Total_Sales = sum(Item_Outlet_Sales)) %>%
  ungroup()
# Merge sales summary with store ages
sales_with_age <- merge(sales_summary, store_ages, by = "Outlet_Identifier")
# Load the ggplot2 library
library(ggplot2)

# Plot total sales by Outlet_ID with Outlet_Age in the x-axis labels
ggplot(sales_with_age, aes(x = Outlet_Identifier, y = Total_Sales)) +
  geom_bar(stat = "identity", fill = "cornflowerblue") +
  scale_y_continuous(labels = scales::comma) +
  labs(title = "Total Sales by Outlet ID with Age Labels",
       x = "Outlet ID (Age)",
       y = "Total Sales") +
  scale_x_discrete(labels = paste(sales_with_age$Outlet_Identifier, "\n(Age:", sales_with_age$Outlet_Age, "yrs)")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))  # Adjust text to make it readable


```
####counting number of sales for store perche mi sembra strano

```{r counting number of sales for each store}

# Sum the total sales for each outlet, grouped by Outlet Identifier and include Outlet Age
sales_summary <- df %>%
  group_by(Outlet_Identifier, Outlet_Age) %>%
  summarise(Total_Sales = sum(Item_Outlet_Sales), .groups = 'drop') %>%
  arrange(desc(Total_Sales))

# Print the summary table with total sales and outlet age
print(sales_summary)
```

```{R one hot econding}
# Identificazione delle variabili categoriche per l'encoding
categorical_vars <- c("Outlet_Type", "Item_Type_Simplified")

# Applicazione del one-hot encoding
dummies <- dummyVars(~ Outlet_Type + Item_Type_Simplified, data = df, fullRank = FALSE)
combined_data_ohe <- predict(dummies, newdata = df)

# Conversione in dataframe
combined_data_ohe <- as.data.frame(combined_data_ohe)

# Rimozione delle variabili categoriche originali e la colonna "Item_Type"
df <- df %>%
  select(-one_of(categorical_vars), -Item_Type) %>%
  bind_cols(combined_data_ohe)

```


Modelli di previsione su:

  -item rmp
  -outlet size(?) o type
  -Percentage of visibility
  -Outlet id


Ora dobbiamo creare dei lower-level model per vedere quali variabili utilizzare nel nostro dataset finale, intuitivamente abbiamo pensato che item rmp, outlet size, item visibility e outlet_id possano essere dei buoni indicatori.

Per fare ciò, proviamo a predicare la nostra variabile target Item_Sales con queste 4 tramite GBM, possiamo usare gbm perchè avendo poche va

Non possiamo usare outlet_ID perchè:

non possiamo trasformarlo in numerico in quanto i valori non seguono un ordin
-I record unici sono solamente 10 quindi il modello avrebbe errori

Possiaom però sostituire Outlet_ID con un'altra variabile ad esempio "Outlet_Age", per vedere se il numero di anni da cui è aperto un negozio inlfuisce su quanto venda un determinato prodotto.



prima di tutto però dobbiamo dividere in test set e train set il nostro modello, useremo 80% in train ed il 20% in test

Poiche non si possono eseguire lm se delle variabili hanno missing values, andremo a rimuovere i missing value/NA rimasti fino ad ora in modo tale da skippare le righe contenenti.

```{R splitting set}
library(caret)

# Impostazione del seed per la riproducibilità
set.seed(123)

# Divisione del dataset in set di training e test
index <- createDataPartition(df$Item_Outlet_Sales, p = 0.8, list = FALSE)
train_data <- df[index, ]
test_data <- df[-index, ]
```


Ora andremo ad eseguire i 4 modelli sul train set


```{R model on the train set}
# Carico le librerie necessarie, se non già caricato
if (!require("ggplot2")) install.packages("ggplot2")
library(ggplot2)

# Assumo che il tuo dataframe si chiami train_data

# Modello per Item_MRP
train_data_MRP <- na.omit(train_data[, c("Item_MRP", "Item_Outlet_Sales")])  # Omette NA solo nelle colonne specificate
model_Item_MRP <- lm(Item_Outlet_Sales ~ Item_MRP, data = train_data_MRP)

# Modello per Outlet_Size
train_data_Size <- na.omit(train_data[, c("Outlet_Size", "Item_Outlet_Sales")])
model_Outlet_Size <- lm(Item_Outlet_Sales ~ Outlet_Size, data = train_data_Size)

# Modello per Item_Visibility
train_data_Visibility <- na.omit(train_data[, c("Item_Visibility", "Item_Outlet_Sales")])
model_Item_Visibility <- lm(Item_Outlet_Sales ~ Item_Visibility, data = train_data_Visibility)

# Modello per Outlet_Age
train_data_Age <- na.omit(train_data[, c("Outlet_Age", "Item_Outlet_Sales")])
model_Outlet_Age <- lm(Item_Outlet_Sales ~ Outlet_Age, data = train_data_Age)

# Stampa i riepiloghi dei modelli
cat("\nRiepilogo del modello per Item MRP:\n")
print(summary(model_Item_MRP))

cat("\nRiepilogo del modello per Outlet Size:\n")
print(summary(model_Outlet_Size))

cat("\nRiepilogo del modello per Item Visibility:\n")
print(summary(model_Item_Visibility))

cat("\nRiepilogo del modello per Outlet Age:\n")
print(summary(model_Outlet_Age))

```

```{R model on the test set}
# Assumo che il tuo dataframe si chiami test_data

# Modello per Item_MRP sul test set
test_data_MRP <- na.omit(test_data[, c("Item_MRP", "Item_Outlet_Sales")])
predictions_MRP <- predict(model_Item_MRP, newdata = test_data_MRP)

# Modello per Outlet_Size sul test set
test_data_Size <- na.omit(test_data[, c("Outlet_Size", "Item_Outlet_Sales")])
predictions_Size <- predict(model_Outlet_Size, newdata = test_data_Size)

# Modello per Item_Visibility sul test set
test_data_Visibility <- na.omit(test_data[, c("Item_Visibility", "Item_Outlet_Sales")])
predictions_Visibility <- predict(model_Item_Visibility, newdata = test_data_Visibility)

# Modello per Outlet_Age sul test set
test_data_Age <- na.omit(test_data[, c("Outlet_Age", "Item_Outlet_Sales")])
predictions_Age <- predict(model_Outlet_Age, newdata = test_data_Age)

# Calcolo delle metriche di performance (RMSE e R-squared) per ogni modello
library(Metrics)

# RMSE
rmse_MRP <- rmse(test_data_MRP$Item_Outlet_Sales, predictions_MRP)
rmse_Size <- rmse(test_data_Size$Item_Outlet_Sales, predictions_Size)
rmse_Visibility <- rmse(test_data_Visibility$Item_Outlet_Sales, predictions_Visibility)
rmse_Age <- rmse(test_data_Age$Item_Outlet_Sales, predictions_Age)

# R-squared
r2_MRP <- summary(lm(Item_Outlet_Sales ~ predictions_MRP, data = test_data_MRP))$r.squared
r2_Size <- summary(lm(Item_Outlet_Sales ~ predictions_Size, data = test_data_Size))$r.squared
r2_Visibility <- summary(lm(Item_Outlet_Sales ~ predictions_Visibility, data = test_data_Visibility))$r.squared
r2_Age <- summary(lm(Item_Outlet_Sales ~ predictions_Age, data = test_data_Age))$r.squared

# Stampa dei risultati
print(data.frame(
  Variable = c("Item_MRP", "Outlet_Size", "Item_Visibility", "Outlet_Age"),
  RMSE = c(rmse_MRP, rmse_Size, rmse_Visibility, rmse_Age),
  R_Squared = c(r2_MRP, r2_Size, r2_Visibility, r2_Age)
))

```
scrivi il chunk in cui vengono comparate le performance del test e del train.




	•	Item_MRP è la variabile più predittiva tra quelle considerate, spiegando circa il 30% della varianza nelle vendite. Questa variabile potrebbe essere utile nel modello finale.
	•	Outlet_Size ha una capacità predittiva molto bassa ma potrebbe ancora fornire qualche informazione utile in un modello multivariato.
	•	Item_Visibility e Outlet_Age hanno una capacità predittiva trascurabile e probabilmente non aggiungono valore significativo a un modello predittivo delle vendite se usate da sole.



Selezione del miglior modello con la stepwise regression


```{R stepwise regression}
if (!require(caret)) install.packages("caret")
if (!require(MASS)) install.packages("MASS")
library(caret)
library(MASS)

# Identificazione e rimozione delle variabili categoriche
train_data <- train_data[, sapply(train_data, is.numeric)]
test_data <- test_data[, sapply(test_data, is.numeric)]
test_data <- na.omit(test_data)  # Rimuove tutte le righe con NA
train_data <- na.omit(train_data)
# Modello iniziale con le variabili numeriche rimanenti
fit_initial <- lm(Item_Outlet_Sales ~ ., data = train_data)

# Selezione delle variabili usando il metodo backward
step_model <- stepAIC(fit_initial, direction = "backward", trace = FALSE)

# Stampa del modello finale dopo la selezione backward
print(summary(step_model))

# Previsione sul test set utilizzando il modello selezionato
predictions <- predict(step_model, newdata = test_data)

# Calcolo delle metriche di performance sul test set
actuals <- test_data$Item_Outlet_Sales
rmse_value <- sqrt(mean((predictions - actuals)^2))
r_squared <- summary(lm(actuals ~ predictions))$r.squared

# Stampa delle metriche di performance
cat("Performance Metrics on Test Set:\n")
cat("RMSE:", rmse_value, "\n")
cat("R-squared:", r_squared, "\n")

# Puoi anche desiderare di vedere come il modello si comporta sul set di training per confronto
train_predictions <- predict(step_model, newdata = train_data)
train_actuals <- train_data$Item_Outlet_Sales
train_rmse <- sqrt(mean((train_predictions - train_actuals)^2))
train_r_squared <- summary(lm(train_actuals ~ train_predictions))$r.squared

# Stampa delle metriche di performance sul train set
cat("Performance Metrics on Train Set:\n")
cat("RMSE:", train_rmse, "\n")
cat("R-squared:", train_r_squared, "\n")

```
```{r Extracting Stepwise Variables}
# Extracting the names of the variables used in the final stepwise model
stepwise_variables <- names(coef(step_model))
stepwise_variables <- c(stepwise_variables, "")
stepwise_variables <- stepwise_variables[-c(1, length(stepwise_variables))]

# Print the list of variables used in the stepwise regression model
print(stepwise_variables)
```
## Ridge Regression
```{R Ridge }
if (!require(glmnet)) install.packages("glmnet")
library(glmnet)
# Preparazione dei dati
x_train <- model.matrix(Item_Outlet_Sales ~ . - 1, data = train_data)
y_train <- train_data$Item_Outlet_Sales

x_test <- model.matrix(Item_Outlet_Sales ~ . - 1, data = test_data)
y_test <- test_data$Item_Outlet_Sales
# Modello Ridge
set.seed(123)
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0, lambda = 10^seq(10, -2, length = 100))
best_lambda_ridge <- ridge_model$lambda.min

# Previsione sul test set
ridge_predictions <- predict(ridge_model, s = best_lambda_ridge, newx = x_test)

# Calcolo delle metriche di performance
ridge_rmse <- sqrt(mean((ridge_predictions - y_test)^2))
ridge_r_squared <- summary(lm(y_test ~ ridge_predictions))$r.squared

cat("Ridge Regression Performance Metrics:\n")
cat("RMSE:", ridge_rmse, "\n")
cat("R-squared:", ridge_r_squared, "\n")
```

```{r Ridge using only the variables selected from stepwise}
if (!require(glmnet)) install.packages("glmnet")
library(glmnet)

formula <- as.formula(paste("Item_Outlet_Sales ~", paste(stepwise_variables, collapse=" + ")))

# Creating the model matrices
x_train <- model.matrix(formula, data=train_data)
y_train <- train_data$Item_Outlet_Sales

x_test <- model.matrix(formula, data=test_data)
y_test <- test_data$Item_Outlet_Sales

# Ridge Regression Model
set.seed(123)
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0, lambda = 10^seq(10, -2, length = 100))
best_lambda_ridge <- ridge_model$lambda.min

# Prediction on the test set
ridge_predictions <- predict(ridge_model, s = best_lambda_ridge, newx = x_test)

# Calculate performance metrics
ridge_rmse <- sqrt(mean((ridge_predictions - y_test)^2))
ridge_r_squared <- summary(lm(y_test ~ ridge_predictions))$r.squared

cat("Ridge Regression Performance Metrics:\n")
cat("RMSE:", ridge_rmse, "\n")
cat("R-squared:", ridge_r_squared, "\n")
```

## Lasso Regression
```{R Lasso Regression}
# Modello Lasso
set.seed(123)
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, lambda = 10^seq(10, -2, length = 100))
best_lambda_lasso <- lasso_model$lambda.min

# Previsione sul test set
lasso_predictions <- predict(lasso_model, s = best_lambda_lasso, newx = x_test)

# Calcolo delle metriche di performance
lasso_rmse <- sqrt(mean((lasso_predictions - y_test)^2))
lasso_r_squared <- summary(lm(y_test ~ lasso_predictions))$r.squared

cat("Lasso Regression Performance Metrics:\n")
cat("RMSE:", lasso_rmse, "\n")
cat("R-squared:", lasso_r_squared, "\n")
```

```{r lasso with only the selected variables by stepwise reg}
if (!require(glmnet)) install.packages("glmnet")
library(glmnet)

formula <- as.formula(paste("Item_Outlet_Sales ~", paste(stepwise_variables, collapse=" + "), "- 1"))

# Creating the model matrices
x_train <- model.matrix(formula, data=train_data)
y_train <- train_data$Item_Outlet_Sales

x_test <- model.matrix(formula, data=test_data)
y_test <- test_data$Item_Outlet_Sales

# Lasso Regression Model
set.seed(123)
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, lambda = 10^seq(10, -2, length = 100))
best_lambda_lasso <- lasso_model$lambda.min

# Prediction on the test set
lasso_predictions <- predict(lasso_model, s = best_lambda_lasso, newx = x_test)

# Calculate performance metrics
lasso_rmse <- sqrt(mean((lasso_predictions - y_test)^2))
lasso_r_squared <- summary(lm(y_test ~ lasso_predictions))$r.squared

cat("Lasso Regression Performance Metrics:\n")
cat("RMSE:", lasso_rmse, "\n")
cat("R-squared:", lasso_r_squared, "\n")
```


##Elastic Net

```{R elastic net}
set.seed(123)
elastic_net_model <- cv.glmnet(x_train, y_train, alpha = 0.5, lambda = 10^seq(10, -2, length = 100))
best_lambda_elastic_net <- elastic_net_model$lambda.min

# Previsione sul test set
elastic_net_predictions <- predict(elastic_net_model, s = best_lambda_elastic_net, newx = x_test)

# Calcolo delle metriche di performance
elastic_net_rmse <- sqrt(mean((elastic_net_predictions - y_test)^2))
elastic_net_r_squared <- summary(lm(y_test ~ elastic_net_predictions))$r.squared

cat("Elastic Net Performance Metrics:\n")
cat("RMSE:", elastic_net_rmse, "\n")
cat("R-squared:", elastic_net_r_squared, "\n")
```

```{r elastic net using only the variables selected by stepwise}
if (!require(glmnet)) install.packages("glmnet")
library(glmnet)

# Using the 'stepwise_variables' list to define the formula for the model matrix
formula <- as.formula(paste("Item_Outlet_Sales ~", paste(stepwise_variables, collapse=" + "), "- 1"))

# Creating the model matrices
x_train <- model.matrix(formula, data=train_data)
y_train <- train_data$Item_Outlet_Sales

x_test <- model.matrix(formula, data=test_data)
y_test <- test_data$Item_Outlet_Sales

# Elastic Net Model
set.seed(123)
elastic_net_model <- cv.glmnet(x_train, y_train, alpha = 0.5, lambda = 10^seq(10, -2, length = 100))
best_lambda_elastic_net <- elastic_net_model$lambda.min

# Prediction on the test set
elastic_net_predictions <- predict(elastic_net_model, s = best_lambda_elastic_net, newx = x_test)

# Calculate performance metrics
elastic_net_rmse <- sqrt(mean((elastic_net_predictions - y_test)^2))
elastic_net_r_squared <- summary(lm(y_test ~ elastic_net_predictions))$r.squared

cat("Elastic Net Performance Metrics:\n")
cat("RMSE:", elastic_net_rmse, "\n")
cat("R-squared:", elastic_net_r_squared, "\n")
```


Il valore R-squared per ogni metodo utilizzato è di circa 0.57, ora abbiamo più modi di procedere (li faremo tutti):

1. Stacking dei diversi metodi utilizzati fino ad ora
2. Ulteriore feature engineering
3. Modelli non lineari come il gadient boosting machines
4. Hyperparameter Tuning
5. La cross-validation

```{R ensembling / stacking}
set.seed(123)
x_train <- model.matrix(Item_Outlet_Sales ~ . - 1, data = train_data)
y_train <- train_data$Item_Outlet_Sales

x_test <- model.matrix(Item_Outlet_Sales ~ . - 1, data = test_data)
y_test <- test_data$Item_Outlet_Sales
# Modello Ridge
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0, lambda = 10^seq(10, -2, length = 100))
ridge_predictions_train <- predict(ridge_model, s = ridge_model$lambda.min, newx = x_train)
ridge_predictions_test <- predict(ridge_model, s = ridge_model$lambda.min, newx = x_test)

# Modello Lasso
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, lambda = 10^seq(10, -2, length = 100))
lasso_predictions_train <- predict(lasso_model, s = lasso_model$lambda.min, newx = x_train)
lasso_predictions_test <- predict(lasso_model, s = lasso_model$lambda.min, newx = x_test)

# Modello Elastic Net
elastic_net_model <- cv.glmnet(x_train, y_train, alpha = 0.5, lambda = 10^seq(10, -2, length = 100))
elastic_net_predictions_train <- predict(elastic_net_model, s = elastic_net_model$lambda.min, newx = x_train)
elastic_net_predictions_test <- predict(elastic_net_model, s = elastic_net_model$lambda.min, newx = x_test)
# Combinare le previsioni dei modelli base per il set di allenamento
stacked_predictions_train <- data.frame(
  Ridge = ridge_predictions_train,
  Lasso = lasso_predictions_train,
  ElasticNet = elastic_net_predictions_train
)

# Combinare le previsioni dei modelli base per il set di test
stacked_predictions_test <- data.frame(
  Ridge = ridge_predictions_test,
  Lasso = lasso_predictions_test,
  ElasticNet = elastic_net_predictions_test
)
# Addestrare il meta-learner (regressione lineare)
meta_learner <- lm(y_train ~ ., data = stacked_predictions_train)

# Previsioni del meta-learner sul set di test
meta_predictions_test <- predict(meta_learner, newdata = stacked_predictions_test)

# Calcolo delle metriche di performance
meta_rmse <- sqrt(mean((meta_predictions_test - y_test)^2))
meta_r_squared <- summary(lm(y_test ~ meta_predictions_test))$r.squared

cat("Stacking Performance Metrics:\n")
cat("RMSE:", meta_rmse, "\n")
cat("R-squared:", meta_r_squared, "\n")
```
head(df)

visto che anche con lo stacking la performance è simile, andiamo ad aggiungere delle nuove feature.

```{R feature engineering}

# Creazione della variabile Price_Per_Unit_Weight
df$Price_Per_Unit_Weight <- ifelse(is.na(df$Item_Weight), 0, df$Item_MRP / df$Item_Weight)


# Creazione della variabile logaritmica per Item_Visibility
df$Log_Item_Visibility <- log(df$Item_Visibility + 1)

# Verifica delle prime righe del dataset per controllare le nuove variabili
head(df)
sum(is.na(df$Item_MRP))
sum(is.na(df$Outlet_Size))
df <- na.omit(df, cols = "Outlet_Size")
df <- df %>%
  filter_all(all_vars(!(is.character(.) & . == "")))
df$Outlet_Size <- as.numeric((df$Outlet_Size))

df$MRP_x_Outlet_Size <- df$Item_MRP * df$Outlet_Size


```









```{R stepwise regression}
if (!require(caret)) install.packages("caret")
if (!require(MASS)) install.packages("MASS")
library(caret)
library(MASS)

# Identificazione e rimozione delle variabili categoriche
train_data <- train_data[, sapply(train_data, is.numeric)]
test_data <- test_data[, sapply(test_data, is.numeric)]
test_data <- na.omit(test_data)  # Rimuove tutte le righe con NA
train_data <- na.omit(train_data)
# Modello iniziale con le variabili numeriche rimanenti
fit_initial <- lm(Item_Outlet_Sales ~ ., data = train_data)

# Selezione delle variabili usando il metodo backward
step_model <- stepAIC(fit_initial, direction = "backward", trace = FALSE)

# Stampa del modello finale dopo la selezione backward
print(summary(step_model))

# Previsione sul test set utilizzando il modello selezionato
predictions <- predict(step_model, newdata = test_data)

# Calcolo delle metriche di performance sul test set
actuals <- test_data$Item_Outlet_Sales
rmse_value <- sqrt(mean((predictions - actuals)^2))
r_squared <- summary(lm(actuals ~ predictions))$r.squared

# Stampa delle metriche di performance
cat("Performance Metrics on Test Set:\n")
cat("RMSE:", rmse_value, "\n")
cat("R-squared:", r_squared, "\n")

# Puoi anche desiderare di vedere come il modello si comporta sul set di training per confronto
train_predictions <- predict(step_model, newdata = train_data)
train_actuals <- train_data$Item_Outlet_Sales
train_rmse <- sqrt(mean((train_predictions - train_actuals)^2))
train_r_squared <- summary(lm(train_actuals ~ train_predictions))$r.squared

# Stampa delle metriche di performance sul train set
cat("Performance Metrics on Train Set:\n")
cat("RMSE:", train_rmse, "\n")
cat("R-squared:", train_r_squared, "\n")

```







Random Forest tree using only selected



```{r random forest tree using only selected variables}
# Install and load the randomForest package
if (!require(randomForest)) {
    install.packages("randomForest", dependencies = TRUE)
}
library(randomForest)

# Assuming 'train_data' and 'test_data' are your datasets
# Make sure all categorical variables are converted to factors if they are not already

# Convert factors (Uncomment and adjust as necessary)
# train_data$Outlet_Type <- as.factor(train_data$Outlet_Type)
# test_data$Outlet_Type <- as.factor(test_data$Outlet_Type)
# Additional categorical variables should be handled similarly.

# Prepare data for Random Forest
train_features <- train_data[, names(train_data) != "Item_Outlet_Sales"]
train_labels <- train_data$Item_Outlet_Sales

test_features <- test_data[, names(test_data) != "Item_Outlet_Sales"]
test_labels <- test_data$Item_Outlet_Sales

# Train the Random Forest model
random_forest_model <- randomForest(x = train_features, y = train_labels, ntree = 200, mtry = 7, importance = TRUE)#mtry da 3 a 18, cambia tree

# Predictions on the test set
rf_predictions <- predict(random_forest_model, newdata = test_features)

# Calculate RMSE and R-squared
rf_rmse <- sqrt(mean((rf_predictions - test_labels)^2))
rf_r_squared <- summary(lm(test_labels ~ rf_predictions))$r.squared

# Print performance metrics
cat("Random Forest Performance Metrics on Test Set:\n")
cat("RMSE:", rf_rmse, "\n")
cat("R-squared:", rf_r_squared, "\n")

# Optionally, print variable importance
print(importance(random_forest_model), type = 1)  # type = 1 for mean decrease in accuracy

```

```{r random forest tree mtry and node size number detection}
# Install and load necessary packages
# Install and load necessary packages
if (!require("caret")) install.packages("caret", dependencies = TRUE)
if (!require("ranger")) install.packages("ranger")
library(caret)
library(ranger)

# Load your data
df <- na.omit(df)

# Prepare train and test sets
set.seed(123)  # For reproducibility
index <- createDataPartition(df$Item_Outlet_Sales, p = 0.8, list = FALSE)
train_data <- df[index, ]
test_data <- df[-index, ]

# Define training control
train_control <- trainControl(
  method = "cv",          # Use cross-validation
  number = 10,            # Number of folds in cross-validation
  savePredictions = "final",
  verboseIter = TRUE
)

# Define the tuning grid
grid <- expand.grid(
  mtry = c(2,18, by=1),                  # Number of variables randomly sampled as candidates at each split
  splitrule = c("variance"),          # Use variance as the split rule for regression
  min.node.size = c(5, 350, by=20)            # Minimum size of terminal nodes
)

# Train the model using Random Forest via 'ranger' for regression
model <- train(
  Item_Outlet_Sales ~ .,             # Regression formula
  data = train_data,                 # Training data
  method = "ranger",                 # Use the 'ranger' package
  trControl = train_control,         # Training control
  tuneGrid = grid,                   # Grid of hyperparameters
  metric = "RMSE",                   # Optimization metric for regression
  maximize = FALSE                   # Minimize the metric (since it's RMSE)
)

# View the best tuning parameters
print(model$bestTune)

# View the results of all tuning iterations
print(model$results)

```
CON NODE SIZE NODE=5 E MTRY = 18 ABBIAMO FINALMENTE TOCCATO IL 0.6 DI RSQUARED NAMOOOO


# Clustering

Abbiamo deciso di fare il clustering su item MRP e outlet type per vedere se il mrp cambia in base al tipo di negozio

## Determining optimal number of clusters using the elbow method

qui dobbiamo calculare il punto di elbow che è il punto più basso in cui la curva decresce, quel punto sarà il punto che ci indicherà il numero di clusters da usare
```{R elbow method}

# Load necessary library
if (!require("stats")) install.packages("stats")
library(stats)

#scaliamo la variabile item_MRP

df$Item_MRP <- scale(df$Item_MRP)

# Perform k-means clustering to determine the optimal number of clusters using the Elbow method
set.seed(123)
wcss <- sapply(1:10, function(k) {
  kmeans(df[, c("Item_MRP", "Outlet_Type.Grocery Store", "Outlet_Type.Supermarket Type1",
                "Outlet_Type.Supermarket Type2", "Outlet_Type.Supermarket Type3")], centers = k, nstart = 25)$tot.withinss
})

# Plotting the Elbow Curve
if (!require("ggplot2")) install.packages("ggplot2")
library(ggplot2)
elbow_plot <- data.frame(k = 1:20, wcss = wcss)
ggplot(elbow_plot, aes(x = k, y = wcss)) +
  geom_line() +
  geom_point() +
  labs(title = "Elbow Method for Determining Optimal k",
       x = "Number of Clusters k",
       y = "Total Within-Cluster Sum of Squares (WCSS)") +
  theme_minimal()
```

il numero di clusters ideale sembra essere 9, facciamo la silhouette analysis epr confermare:


```{R silhouette analysis}
# K-means con k = 9, riconfermto come ottimale
optimal_k <- 2
final_kmeans_result <- kmeans(df[, c("Item_MRP", "Outlet_Type.Grocery Store", "Outlet_Type.Supermarket Type1",
                                     "Outlet_Type.Supermarket Type2", "Outlet_Type.Supermarket Type3")],
                              centers = optimal_k, nstart = 50)

# Visualizzazione dei cluster finali
library(ggplot2)
df$Cluster <- final_kmeans_result$cluster
ggplot(df, aes(x = factor(Cluster), y = Item_MRP, fill = factor(Cluster))) +
  geom_boxplot() +
  labs(title = "Distribution of Item MRP Across Clusters",
       x = "Cluster",
       y = "Item MRP") +
  scale_fill_brewer(palette = "Set3") +
  theme_minimal()
```
```{r k-means clustering}
# Assuming the elbow point is at k = 9 based on the plot
optimal_k <- 2
kmeans_result <- kmeans(df[, c("Item_MRP", "Outlet_Type.Grocery Store", "Outlet_Type.Supermarket Type1",
                               "Outlet_Type.Supermarket Type2", "Outlet_Type.Supermarket Type3")], centers = optimal_k, nstart = 25)

# Add cluster labels to the original data
df$Cluster <- kmeans_result$cluster

# Summary by cluster
summary <- aggregate(cbind(Item_MRP) ~ Cluster, data = df, FUN = mean)
print(summary)
```
```{r plotting the cluster}
# Plotting the clusters
ggplot(df, aes(x = Cluster, y = Item_MRP, color = factor(Cluster))) +
  geom_jitter(alpha = 0.5) +
  facet_wrap(~ Cluster) +
  labs(title = "Cluster of Item MRP across Outlet Types",
       x = "Cluster",
       y = "Scaled Item MRP",
       color = "Cluster") +
  theme_minimal()
```


Verifica se certi tipi di prodotti sono più concentrati in uno dei due cluster. Vediamo ancora una volta se i prodotti che vendono di più sono quelli nel cluster 2(high MRP) o 1 (Low MRP). In teoria dovrebbero essere in quelli con l'MRP più alto


```{r item outlet sales cluster mrp}
if (!require("ggplot2")) install.packages("ggplot2", dependencies = TRUE)
library(ggplot2)

# 1. Statistiche Descrittive per Item_Outlet_Sales per Cluster
sales_stats <- aggregate(Item_Outlet_Sales ~ Cluster, data = df, FUN = function(x) c(mean = mean(x), sd = sd(x), median = median(x), IQR = IQR(x)))
print(sales_stats)

# 2. Test di Significatività tra i due Cluster
# Controlla prima che entrambi i cluster abbiano più di un elemento
if (any(table(df$cluster) > 1)) {
    t_test_results <- t.test(Item_Outlet_Sales ~ cluster, data = df)
    print(t_test_results)
}

# 3. Visualizzazione con Box Plot
ggplot(df, aes(x = factor(Cluster), y = Item_Outlet_Sales, fill = factor(Cluster))) +
  geom_boxplot() +
  labs(title = "Distribution of Item Outlet Sales Across Clusters",
       x = "Cluster",
       y = "Item Outlet Sales",
       fill = "Cluster") +
  theme_minimal()

```
Tesi confermata :)


Ora vediamo quali tipi di supermercato rientrano nella fascia che costa tanto e quali nella fascia che costa poco.

Seguendo un'analisi puramente economica, i Grocery Store ed i Supermarket Type 1, dovranno stare nel cluster con i prezzi alti in quanto hanno pochi prodotti e quindi poca concorrenza interna

```{R outlet type vs cluster mrp}
# Rinomina le colonne rimuovendo gli spazi
names(df) <- gsub(" ", "", names(df))

# Verifica i nomi delle colonne aggiornati
print(names(df))
# Ricrea la variabile Outlet_Type dal one-hot encoding
df$Outlet_Type <- ifelse(df$Outlet_Type.GroceryStore == 1, "Grocery Store",
                         ifelse(df$Outlet_Type.SupermarketType1 == 1, "Supermarket Type1",
                         ifelse(df$Outlet_Type.SupermarketType2 == 1, "Supermarket Type2",
                         ifelse(df$Outlet_Type.SupermarketType3 == 1, "Supermarket Type3", NA))))

# Controlla che la nuova variabile sia stata creata correttamente
table(df$Outlet_Type)

# Crea una tabella di contingenza tra i cluster di prezzo e i tipi di outlet
outlet_price_cluster_table <- table(df$Outlet_Type, df$Cluster)
print(outlet_price_cluster_table)

# Visualizzazione con barplot
barplot(outlet_price_cluster_table, beside = TRUE, legend = TRUE, 
        col = c("red", "blue"), 
        main = "Distribution of Outlet Types Across Price Clusters",
        ylab = "Count",
        xlab = "Outlet Type",
        args.legend = list(title = "Cluster", x = "topright"))

# Per una visualizzazione più dettagliata potresti utilizzare ggplot2
library(ggplot2)
df_long <- reshape2::melt(outlet_price_cluster_table)
ggplot(df_long, aes(x = Var1, y = value, fill = Var2)) +
    geom_bar(stat = "identity", position = position_dodge()) +
    labs(x = "Outlet Type", y = "Count", fill = "Cluster") +
    ggtitle("Distribution of Outlet Types Across Price Clusters") +
    theme_minimal()

```
La distribuzione quasi uniforme dei tipi di outlet tra i due cluster di prezzi suggerisce che non c'è un tipo di outlet che domina un particolare cluster di prezzo. Questo può indicare che la strategia di prezzo o la gamma di prodotti potrebbe essere simile tra i vari tipi di outlet o che la segmentazione dei prezzi non è fortemente influenzata dal tipo di outlet.

UN ULTIMA ANALISI CHE FACCIAMO E' TRA ITEM MRP E PRODOTTI GRASSI/NON GRASSI.


```{R cluster on item fat content}
# Crea sottoinsiemi per Regular e Low Fat considerando la codifica numerica
df_regular <- df[df$Item_Fat == 2, ]  # Regular Fat
df_low <- df[df$Item_Fat == 1, ]      # Low Fat

# Statistiche per prodotti Regular e Low Fat in ciascun cluster
regular_stats <- aggregate(Item_MRP ~ Cluster, data = df_regular, FUN = mean)
low_fat_stats <- aggregate(Item_MRP ~ Cluster, data = df_low, FUN = mean)

print(regular_stats)
print(low_fat_stats)

# Test t per confrontare i prezzi tra Regular Fat (2) e Low Fat (1) in ciascun cluster
t_test_cluster1 <- t.test(Item_MRP ~ Item_Fat_Content, data = df[df$Cluster == '1', ], subset = Item_Fat_Content %in% c(1, 2))
t_test_cluster2 <- t.test(Item_MRP ~ Item_Fat_Content, data = df[df$Cluster == '2', ], subset = Item_Fat_Content %in% c(1, 2))

print(t_test_cluster1)
print(t_test_cluster2)

# Visualizzazione con ggplot2
library(ggplot2)
ggplot(df, aes(x = factor(Item_Fat_Content), y = Item_MRP, fill = factor(Item_Fat_Content))) +
  geom_boxplot() +
  facet_wrap(~Cluster) +
  scale_fill_manual(values = c("blue", "red"), labels = c("Low Fat", "Regular Fat")) +
  labs(title = "Distribution of Item MRP by Item Fat Content Across Clusters",
       x = "Item Fat Content",
       y = "Item Maximum Retail Price (MRP)",
       fill = "Item Fat") +
  theme_minimal()

```


scrivi interpret di questo